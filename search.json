[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSP Lecture Notes",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_Intro.html",
    "href": "01_Intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html",
    "href": "04_FourierTransform.html",
    "title": "2  The Fourier Transform",
    "section": "",
    "text": "2.1 Introduction: vector spaces of signals",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#introducing-the-fourier-transforms",
    "href": "04_FourierTransform.html#introducing-the-fourier-transforms",
    "title": "2  The Fourier Transform",
    "section": "2.2 Introducing the Fourier Transforms",
    "text": "2.2 Introducing the Fourier Transforms\nLet’s start with a reminder of some basic mathematical formulas.\n\nthe Euler formula: \\[\n  e^{jx} = \\cos(x) + j \\sin(x)\n\\]\nThis function \\(e^{jx}\\) is known as the complex exponential.\nrelation between \\(\\cos()\\) and \\(\\sin()\\) and the complex exponential: \\[\n\\begin{split}\n  \\cos(x) &= \\frac{e^{jx} + e^{-jx}}{2}\\\\\n  \\sin(x) &= \\frac{e^{jx} - e^{-jx}}{2j}\\\\\n\\end{split}\n\\]\n\\(\\cos()\\) and \\(\\sin()\\) are just shifted versions of each other: \\[\n  \\begin{split}\n  \\sin(x) &= \\cos(x - \\frac{\\pi}{2})\\\\\n  \\cos(x) &= \\sin(x + \\frac{\\pi}{2})\n  \\end{split}\n\\]\n\n\n2.2.1 Eigen-signals of LTI systems\nWhy are sinusoidal signals \\(\\sin()\\) and \\(\\cos()\\) so prevalent in signal processing? The reason is that they can be written based on \\(e^{jx}\\) and an \\(e^{-jx}\\). In fact, it is the function \\(e^{jx}\\) that is very special, due to its relation to differential equations, and \\(\\sin()\\) and \\(\\cos()\\) merely inherit the nice properties of \\(e^{jx}\\).\nWhy is the function \\(e^{jx}\\) special? From the point of view of LTI systems, it is because it is an eigen-function of these systems.\nAn eigen-function (“funcție proprie”) of a mathematical system is function \\(f\\) which, if input in a system, produces an output proportional to it: \\[H\\lbrace f \\rbrace = \\lambda \\cdot f, \\lambda \\in \\mathbb{C}\\]\nUsing the signal processing terminology, a signal \\(x[n]\\) is called an eigen-signal of the system if the output signal is proportional to the input signal: \\[y[n] = H \\lbrace x[n] \\} = \\lambda \\cdot x[n]\\] It turns out that the complex exponential signals \\(e^{j\\omega n}\\) are eigen-signals of Linear and Time Invariant (LTI) systems.\n\n\n\n\n\n\nTheorem: Complex exponentials are eigen-signals of LTI systems\n\n\n\nThe complex exponential signals \\(e^{j\\omega n}\\) are eigen-functions of Linear and Time Invariant (LTI) systems: \\[\ny[n] = H \\lbrace x[n] \\} = \\lambda \\cdot x[n], \\lambda \\in \\mathbb{C}, \\forall H{} \\textrm{ a LTI system}\n\\]\nProof: Let’s consider an input signal \\(x[n] = A e^{j\\omega_0 n}\\), for some values \\(A\\), \\(\\omega_0 \\in \\mathbb{R}\\).\nThe LTI system \\(H\\) has an impulse response \\(h[n]\\). The output signal is the convolution of the input signal \\(x[n]\\) with the impulse response \\(h[n]\\): \\[\n\\begin{split}\n    y[n] &= \\sum_{k=-\\infty}^\\infty h[k] x[n-k]\\\\\n         &= \\sum_{k=-\\infty}^\\infty h[k] A e^{j \\omega_0 (n-k)} \\\\\n         &= \\sum_{k=-\\infty}^\\infty h[k] e^{-j \\omega_0 k} A e^{j \\omega_0 n}\\\\\n         &= A e^{j \\omega_0 n} \\underbrace{\\sum_{k=-\\infty}^\\infty h[k] e^{-j \\omega_0 k}}_{H(\\omega_0)}\\\\\n         &= H(\\omega_0) \\cdot x[n]\n\\end{split}\n\\]\nWith the notation \\(H(\\omega_0) = \\sum_{k=-\\infty}^\\infty h[k] e^{-j \\omega_0 k}\\), which is a constant complex number, we have shown that \\(y[n]\\) is proportional to \\(x[n]\\): \\[\n    y[n] = H(\\omega_0) \\cdot x[n]\n\\]\n\n\nThe concept of eigen-signals and, more generally, of eigen-functions, is similar to the concept of eigen-vectors of a matrix (remember algebra). Eigen-vectors are vectors which, when multiplied by a matrix \\(A\\), produce a vector proportional to the input vector. \\[\n    A \\cdot v = \\lambda \\cdot v\n\\] The core idea is the same: the vector \\(v\\) is not changed by the matrix \\(A\\) except by scaling with some value.\nEigen-signals are very useful as building blocks of signals. If we can decompose a signal into a sum of eigen-signals, we can easily understand how the signal will be transformed by LTI a system, since each of the eigen-signals will be transformed into a scaled version of itself. The output signal will be a sum of scaled versions of the input signal’s eigen-signals.\nThis is exactly the idea behind the Fourier Transform. The Fourier Transform decomposes a signal into a sum of complex exponentials \\(e^{j\\omega n}\\), so that when the signal passes through an LTI system, we can understand the effect as simply scaling each of the complex exponentials (multiplication by the transfer function \\(H(\\omega)\\)).\n\n\n\n\n\n\nExample\n\n\n\n2.2.1.1 Example\nImagine a professional DSLR photo camera with some RGB color filters mounted on the lens. Suppose we have three photographic filters which reduce the intensity of the red, green, and blue colors:\n\none filter reduces red to 50%: \\[R_{out} = 0.5 \\cdot R_{in}\\]\none filter reduces green to 25%: \\[G_{out} = 0.25 \\cdot G_{in}\\]\none filter reduces blue to 80%: \\[B_{out} = 0.8 \\cdot B_{in}\\]\n\nThe colors R, G, B behave like eigen-colors of the system, in the sense that the output is proportional to the input.\nNow, suppose we have an input color “pink” which is some combination of red and blue. What is the output color if we pass the “pink” color through the filters?\nThe answer is easy if we represent the “pink” color in terms of the eigen-colors R, G, B. If, say, pink is equal to \\(200 \\cdot R + 0 \\cdot G + 200 \\cdot B\\), then the output will simply be: \\[\n200 \\cdot 0.5 \\cdot R + 0 \\cdot 0.25 \\cdot G + 200 \\cdot 0.8 \\cdot B\n\\]\nDecomposing the color in terms of the eigen-colors R, G, B, makes it easy to understand the effect of the filters. R, G, B are the natural way of representing colors in this case, making it easy to understand the effect of the filters.\n\n\n\n\n2.2.2 The Fourier Transforms\nThe Fourier Transform is the mathematical tool that allows us to decompose a signal as a linear combination of complex exponentials \\(e^{j\\omega n}\\).\nFor non-periodic signals, we use the Discrete-Time Fourier Transform (DTFT) and its inverse.\n\nInverse Discrete-Time Fourier Transform (Inverse DTFT):\nA signal \\(x[n]\\) can be written as an infinite sum (i.e. integral) of complex exponentials: \\[x[n] = \\int_{f=-1/2}^{1/2} X(f) e^{j 2 \\pi f n} df\\] with some coefficients, \\(X(f)\\). \nDiscrete-Time Fourier Transform (DTFT):\nThe coefficients \\(X(f)\\) are found as: \\[X(f) = \\langle x[n], e^{j 2 \\pi f n} \\rangle = \\sum_{n=-\\infty}^{\\infty} x[n] e^{- j 2 \\pi f n}\\]\n\nAlternatively, we can write these in terms of \\(\\omega\\), by replacing \\(f\\) with \\(\\omega = 2 \\pi f\\) and \\(df = \\frac{1}{2 \\pi} d\\omega\\): \\[\n\\begin{split}\n    x[n] &= \\frac{1}{2 \\pi}\\int_{\\omega=-\\pi}^{\\pi} X(\\omega) e^{j \\omega n} d\\omega\\\\\n    X(\\omega) &= \\langle x[n], e^{j \\omega n} \\rangle = \\sum_n x[n] e^{- j \\omega n}\n\\end{split}\n\\]\n\nFor periodic signals, we use the Discrete Fourier Transform (DFT) formula and its inverse, because the spectrum \\(X(f)\\) is discrete, so there is just a finite number of coefficients:\n\nInverse Discrete Fourier Transform (Inverse DFT):\nA periodic signal \\(x[n]\\) can be written as a sum of exactly \\(N\\) complex exponentials: \\[ x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k e^{j 2 \\pi k n / N} \\] with some coefficients, \\(X_k\\).\nDiscrete Fourier Transform (DFT):\nThe coefficients \\(X_k\\) are found as: \\[X_k = \\langle x[n], e^{j 2 \\pi f n} \\rangle = \\sum_{n=0}^{N-1} x[n] e^{- j 2 \\pi k n / N}\\]\n\nIn the following, we shall analyze each variant of the Fourier Transform in more detail.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#the-discrete-time-fourier-transform-dtft",
    "href": "04_FourierTransform.html#the-discrete-time-fourier-transform-dtft",
    "title": "2  The Fourier Transform",
    "section": "2.3 The Discrete-Time Fourier Transform (DTFT)",
    "text": "2.3 The Discrete-Time Fourier Transform (DTFT)\nThe Discrete-Time Fourier Transform (DTFT) is used for discrete signals, infinitely long, that are non-periodic.\nInverse Discrete-Time Fourier Transform (Inverse DTFT): \\[x[n] = \\int_{f=-1/2}^{1/2} X(f) e^{j 2 \\pi f n} df = \\frac{1}{2\\pi} \\int_{\\omega=-\\pi}^{\\pi} X(\\omega) e^{j \\omega n} d\\omega\\]\nDiscrete-Time Fourier Transform (DTFT): \\[X(f) = \\langle x[n], e^{j 2 \\pi f n} \\rangle = \\sum_{n=-\\infty}^{\\infty} x[n] e^{- j 2 \\pi f n}\\]\nThe inverse DTFT shows that a signal can be written as a continuous sum (i.e. an intergal) of complex exponentials \\(e^{j \\omega n}\\), with some coefficients \\(X(\\omega)\\) or \\(X(f)\\). The direct DTFT shows how to compute these coefficients.\n\n2.3.1 Basic properties of DTFT\nThe function \\(X(\\omega)\\) is known as the spectrum of the signal \\(x[n]\\).\n\n\\(X(\\omega)\\) is defined only for \\(\\omega \\in [-\\pi, \\pi]\\), or \\(f \\in [-\\frac{1}{2}, \\frac{1}{2}]\\). This is unlike the spectrum of continuous signals, which ranges from \\(-\\infty\\) to \\(\\infty\\).\n\\(X(\\omega)\\) is has complex values, meaning there exists the functions \\(| X(\\omega) |\\) and \\(\\angle X(\\omega)\\).\nIf the signal \\(x[n]\\) is real, \\(X(\\omega)\\) is even: \\[x[n] \\in \\mathbb{R} \\rightarrow X(-\\omega) = X^*(\\omega)\\]\nFurthermore, this means that the modulus \\(|X(\\omega)|\\) is an even, real-valued function: \\[|X(\\omega)| = |X(-\\omega)|\\] and the phase \\(\\angle X(\\omega)\\) is an odd, real-valued function: \\[X(\\omega) = - X(-\\omega)\\]\n\n\n\n2.3.2 Sum of sinusoids\nThe Inverse DTFT can be rewritten to show the signal \\(x[n]\\) as a sum of sinusoids.\nBy grouping terms with \\(e^{j \\omega n}\\) and \\(e^{j (-\\omega) n}\\) we get:\n\\[\n\\begin{split}\n    x[n] &= \\frac{1}{2\\pi} \\int_{-\\pi}^0 X(\\omega) e^{j \\omega n} + \\frac{1}{2\\pi} \\int_0^\\pi X(\\omega) e^{j \\omega n} d\\omega\\\\\n        &= \\frac{1}{2\\pi} \\int_0^\\pi (X(\\omega) e^{j \\omega n}  + X(-\\omega) e^{j (-\\omega) n} ) d\\omega\\\\\n        &= \\frac{1}{2\\pi} \\int_0^\\pi 2|X(\\omega)| ( e^{j \\omega n + \\angle{X(\\omega)}}  +  e^{- j \\omega n - \\angle{X(\\omega)}} ) d\\omega\\\\\n        &= \\frac{1}{2\\pi} \\int_0^\\pi 2 |X(\\omega)| \\cos(\\omega n + \\angle X(\\omega))d\\omega\n\\end{split}\n\\]\nThe Inverse DTFT shows, therefore, that any signal \\(x[n]\\) can be written as a continuous sum (i.e. integral) of sinusoids with all frequencies \\(f \\in [-\\frac{1}{2}, \\frac{1}{2}]\\).\nThe coefficient of each sinusoid is given by the spectrum \\(X(f)\\), for every value of \\(f\\):\n\nThe modulus \\(|X(\\omega)|\\) gives the amplitude of the sinusoids (\\(\\times\\) 2)\n\nAs a particular case for \\(\\omega = 0\\), \\(|X(\\omega=0)|\\) gives the DC component of the signal\n\nThe phase \\(\\angle X(\\omega)\\) gives the initial phase shift of the sinusoids\n\nThis is the fundamental practical interpretation of the Fourier transform: it shows that any signal can be decomposed into a continuous sum of sinusoids of all frequencies, each with a certain amplitude and phase.\n\n\n2.3.3 Properties of DTFT\n\n2.3.3.1 Linearity\nThe DTFT is a linear operation. The DTFT of a linear combination of signals is the linear combination of their DTFTs: \\[a \\cdot x_1[n] + b\\cdot x_2[n] \\leftrightarrow a \\cdot X_1(\\omega)+ b\\cdot X_2(\\omega)\\]\nProof: via definition\n\n\n2.3.3.2 Shifting in time\nThe DTF of a signal delayed by \\(n_0\\) is the DTFT of the original signal, multiplied by a complex exponential:\n\\[x[n - n_0] \\leftrightarrow e^{-j \\omega n_0} X(\\omega)\\]\nProof: via definition\nNote that \\[ | e^{-j \\omega n_0} X(\\omega) | = | X(\\omega) |\\] which shows that when a signal is shifted in time, the amplitudes \\(|X(\\omega)|\\) of the spectrum are not affected. Shifting in time affects only the phase of the spectrum. This makes sense, because shifting a signal in time does not change the “amplitudes” of the composing sinusoids, it only shifts them.\n\n\n2.3.3.3 Modulation in time\nA shift of the spectrum \\(X(\\omega)\\) by \\(\\omega_0\\) corresponds to a modulation of the signal \\(x[n]\\) by a complex exponential:\n\\[e^{j \\omega_0 n} x[n]\\leftrightarrow X(\\omega - \\omega_0)\\]\n\n\n2.3.3.4 Complex conjugation\nComplex conjugation in time domain corresponds in frequency domain to complex conjugation and inversion of the frequency:\n\\[x^*[n] \\leftrightarrow X^*(-\\omega)\\]\n\n\n2.3.3.5 Convolution\nThe convolution of two signals in time domain corresponds to the multiplication of their spectra in frequency domain:\n\\[x_1[n] \\star x_2[n] \\leftrightarrow X_1(\\omega) \\cdot X_2(\\omega)\\]\nProof: TBD\n\n\n2.3.3.6 Product in time\nThe product of two signals in time domain corresponds to the convolution of their spectra in frequency domain:\n\\[x_1[n] \\cdot  x_2[n] \\leftrightarrow \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi X_1(\\lambda) X_2(\\omega - \\lambda) d\\lambda\\]\n\n\n2.3.3.7 Correlation theorem\nThe cross-correlation of two signals in time domain corresponds to the product of their spectra in frequency domain, the second spectrum being complex conjugated:\n\\[r_{x_1x_2}[l] \\leftrightarrow X_1(\\omega) X_2^*(\\omega)\\]\n\n\n2.3.3.8 Wiener-Khinchin theorem\nThe autocorrelation of a signal in time domain corresponds to the power spectral density in frequency domain:\n\\[r_{xx}[l] \\leftrightarrow S_{xx}(\\omega) = |X(\\omega)|^2\\]\nNote that this is a special case of the correlation theorem, where the two signals are the same: \\[r_{xx}[l] \\leftrightarrow X(\\omega) X^*(\\omega) = |X(\\omega)|^2\\] since for any complex number \\(z\\), \\(z \\cdot z^* = |z|^2\\).\n\n\n2.3.3.9 Parseval theorem\nThe Parseval theorem states that the energy of the signal is the same in time and frequency domains: \\[E = \\sum_{-\\infty}^\\infty |x[n]|^2 = \\frac{1}{2 \\pi}\\int_{-\\pi}^\\pi |X(\\omega)|^2\\]\nThe energy of a function, in general, is defined as the sum of the squares of its values. In the case of a signal, in the time domain we have the sum of the squares of the samples, and in the frequency domain we have the integral of the square of the spectrum, since the spectrum is continuous.\n\n\n\n2.3.4 Relation between DTFT and Z transform\nThe DTFT is a special case of the Z transform, where the Z transform is evaluated on the unit circle, i.e. with \\[z = e^{j \\omega}\\]\nThis is clearly seen from the definitions of the DTFT and Z transforms, where replacing \\(z = e^{j \\omega}\\) in the Z transform leads to the DTFT:\n\nZ transform: \\[X(z) = \\sum_n x[n] z^{-n}\\]\nDTFT: \\[X(\\omega) = \\sum_n x[n] e^{-j \\omega n}\\]\n\nThe points defined by \\(z = e^{j \\omega}\\) are points on the unit circle, since the modulus of such \\(z\\) is 1: \\[|z| = |e^{j \\omega}| = 1\\] The phase (angle) of these points \\(z\\) is \\(\\omega\\): \\[\\angle{z} = \\angle{e^{j \\omega}} = \\omega\\]\nWe say therefore that the DTFT is the Z transform evaluated on the unit circle. This requires that the Z transform converges on the unit circle, which is the case for most usual signals. Otherwise, the equivalence does not hold",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#the-discrete-fourier-transform-dft",
    "href": "04_FourierTransform.html#the-discrete-fourier-transform-dft",
    "title": "2  The Fourier Transform",
    "section": "2.4 The Discrete Fourier Transform (DFT)",
    "text": "2.4 The Discrete Fourier Transform (DFT)\nThe Discrete Fourier Transform (DFT) is used for discrete signals, periodic, with period \\(N\\).\nIf we apply the DTFT to a periodic signal, we get a spectrum that is discrete, with only \\(N\\) Dirac deltas. In this case it is easier to replace the integral with a sum of exactly \\(N\\) terms, and in this way we get the DFT.\nInverse Discrete Fourier Transform (Inverse DFT) \\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k e^{j 2 \\pi k n / N}\\]\nDiscrete Fourier Transform (DFT): \\[X_k = \\langle x[n], e^{j 2 \\pi f n} \\rangle = \\sum_{n=0}^{N-1} x[n] e^{- j 2 \\pi k n / N}\\]\nThe DFT is defined for periodical signals with period \\(N\\), and there are exactly \\(N\\) terms in each sum in the two formulas above. The DFT formula only uses the values from one period of the signal, \\(x[n]\\) for \\(n = 0, 1, \\dots, N-1\\), because the signal is periodic and the values repeat from \\(N\\) onwards.\n\n\n\n\n\n\nNote\n\n\n\nRemember the differences between the DTFT and DFT in terms of the types of signals and spectra:\nThe DFT takes a vector with N elements (\\(x[0], \\dots, x[N-1]\\)) and produces a vector with N elements (\\(X_k\\)). For this reason, we can compute it with tools like Matlab, because it is a finite operation.\nThe DTFT takes an infinitely-long signal \\(x[n]\\) and produces a continuous function (\\(X(\\omega)\\)) between \\([-\\pi, \\pi]\\).\n\n\n\n2.4.1 Basic properties of DFT\nThe DFT produces only \\(N\\) coefficients \\(X_k\\), with each \\(X_k\\) corresponding to a frequency \\(f = \\frac{k}{N}\\). \\(X_0\\) corresponds to the DC component, \\(X_1\\) to the frequency \\(f = \\frac{1}{N}\\), and so on.\nThe DFT coefficients \\(X_k\\) are complex numbers, meaning they have a modulus \\(|X_k|\\) and a phase \\(\\angle X_k\\).\nIf the signal \\(x[n] \\in \\mathbb{R}\\), the coefficients are even (and complex): \\[X_{-k} = X_k^*\\] Furthermore, this means that the modulus \\(|X_k|\\) are even, real values: \\[|X_{-k}| = |X_k|\\] and the phase \\(\\angle X_k\\) are odd, real values: \\[\\angle X_{-k} = -\\angle X_k\\]\nThe DFT coefficients \\(X_k\\) are periodic with period \\(N\\), i.e. \\(X_{k+N} = X_k\\). This can be shown from the DFT formula, where, if we replace \\(k\\) with \\(k + N\\), we get the same value: \\[\n\\begin{split}\n    X_{k+N} &= \\sum_{n=0}^{N-1} x[n] e^{- j 2 \\pi (k + N) n / N}\\\\\n            &= \\sum_{n=0}^{N-1} x[n] e^{- j 2 \\pi k n / N} e^{- j 2 \\pi n}\\\\\n            &= \\sum_{n=0}^{N-1} x[n] e^{- j 2 \\pi k n / N}\\\\\n            &= X_k\n\\end{split}\n\\] The periodicity of the DFT coefficients is the reason for which the Inverse DFT only uses the values \\(X_0, X_1, \\dots, X_{N-1}\\), since the values repeat afterwards.\nBecause of periodicity, we can rename the coefficients \\(X_{N-k}\\) as \\(X_{-k}\\). Consider, for example, a signal \\(x[n]\\) with period \\(N = 6\\). The signal has 6 DFT coefficients \\(X_0\\), \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\), \\(X_5\\). However, we can rename the last ones as \\(X_5 = X_{-1}\\), and \\(X_4 = X_{-2}\\). Thus, the 6 coefficients can be considered \\(X_{-2}\\), \\(X_{-1}\\), \\(X_0\\), \\(X_1\\), \\(X_2\\), \\(X_3\\).\nThis “renaming” of coefficients corresponds to the fact that the frequency \\(f = \\frac{N-k}{N}\\) is the same as \\(f - 1 = \\frac{-k}{N}\\), which we know from the aliasing effect. Thus, for a periodic signal with \\(N=6\\), the coefficient \\(X_5\\) corresponds to frequency \\(f = \\frac{5}{6}\\), which is the same as \\(f = \\frac{-1}{6}\\), which corresponds to \\(X_{-1}\\). Similarly, \\(X_4\\) corresponds to \\(f = \\frac{4}{6}\\), which is the same as \\(f = \\frac{-2}{6}\\), which corresponds to \\(X_{-2}\\). The resulting DFT coefficients are \\(X_{-2}\\), \\(X_{-1}\\), \\(X_0\\), \\(X_1\\), \\(X_2\\), \\(X_3\\), and now their corresponding frequencies are all in the range \\([-\\frac{1}{2}, \\frac{1}{2}]\\).\n\n\n2.4.2 Sum of sinusoids\nJust like the DTFT, the DFT can be rewritten to show the signal \\(x[n]\\) as a sum of sinusoids, where the modulus \\(|X_k|\\) gives the amplitude of the sinusoids, and the phase \\(\\angle X_k\\) gives the initial phase shift.\nFor easy analysis, we consider seperately the cases when \\(N\\) is odd and when \\(N\\) is even.\n\n2.4.2.1 Sum of sinusoids with N = odd\nIf \\(N\\) is odd, we have an odd number of coefficients \\(X_k\\). We keep \\(X_0\\) single, and we group together the coefficients \\(X_k\\) and \\(X_{-k}\\) as follows:\n\\[\\begin{split}\nx[n] &= \\sum_{k=-(N-1)/2}^{(N-1)/2} X_k e^{j 2 \\pi k n / N}\\\\\n    &= \\frac{1}{N} X_0 e^{j 0 n} + \\frac{1}{N} \\sum_{k=-(N-1)/2}^{-1} X_k e^{j 2 \\pi k n / N} + \\frac{1}{N} \\sum_{k=1}^{(N-1)/2} X_k e^{j 2 \\pi k n / N}\\\\\n    &= \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{(N-1)/2} (X_k e^{j 2 \\pi k n / N}  + X_{-k} e^{- j 2 \\pi k n / N} )\\\\\n    &= \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{(N-1)/2} |X_k| ( e^{j 2 \\pi k n /N + \\angle{X(k)}}  +  e^{- j 2 \\pi k n / N - \\angle{X(\\omega)}} )\\\\\n    &= \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=0}^{(N-1)/2} 2 |X_k| \\cos(2 \\pi k/N n + \\angle X_k)\n\\end{split}\\]\nThis shows that a signal \\(x[n]\\) with period \\(N\\) can be written as a sum of a few sinusoids with frequencies \\(0, \\frac{1}{N}, \\frac{2}{N}, \\dots\\), not exceeding 1/2.\nThe DC component is given by \\(X_0\\). The amplitudes of the sinusoids are given by \\(|X_k|\\), and the phases by \\(\\angle X_k\\).\n\n\n2.4.2.2 Sum of sinusoids with N = even\nIf \\(N\\) is even, we have an even number of coefficients \\(X_k\\). We group together the coefficients \\(X_k\\) and \\(X_{-k}\\) as follows: - we leave \\(X_0\\) on its own - we group \\(X_1\\) and \\(X_{-1}\\), \\(X_2\\) and \\(X_{-2}\\), and so on - there is one remaining coefficient \\(X_{N/2}\\), which has no pair\nFor example, for \\(N=6\\), we have coefficients \\(X_{-2}, X_{-1}, X_0, X_1, X_2, X_3\\). We group \\(X_1\\) and \\(X_{-1}\\), \\(X_2\\) and \\(X_{-2}\\), while \\(X_0\\) and \\(X_3\\) are left on their own.\nWe observe that the final term \\(X_{N/2}\\), having no pair, must be a real number. Because of periodicity, we must have $X_{N/2} = X_{-N/2}, but on other hand toe coefficients are even, so \\(X_{N/2} = X_{-N/2}^*\\). This means that \\(X_{N/2}\\) is equal to its own complex conjugate, so it must be a real number.\nWe have:\n\\[\\begin{split}\nx[n] &= \\sum_{k=-(N-2)/2}^{N/2} X_k e^{j 2 \\pi k n / N}\\\\\n     &= \\frac{1}{N} X_0 e^{j 0 n} + \\frac{1}{N} \\sum_{k=-(N-2)/2 }^{-1} X_k e^{j 2 \\pi k n / N} + \\frac{1}{N} \\sum_{k=1}^{(N-2)/2} X_k e^{j 2 \\pi k n / N} + \\frac{1}{N} X_{N/2} e^{j 2 \\pi (N/2) n / N}\\\\\n     &= \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{(N-2)/2} (X_k e^{j 2 \\pi k n / N}  + X_{-k} e^{- j 2 \\pi k n / N} ) + \\frac{1}{N} X_{N/2} e^{j 2 \\pi (N/2) n / N}\\\\\n     &= \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{(N-2)/2} 2 |X_k| \\cos(2 \\pi k/N n + \\angle X_k) + \\frac{1}{N} X_{N/2} \\cos(n \\pi)\n\\end{split}\\]\nThis shows that a signal \\(x[n]\\) with period \\(N\\) can be written as a sum of a few sinusoids with frequencies \\(0, \\frac{1}{N}, \\frac{2}{N}, \\dots\\), up to but not exceeding 1/2.\n\nThe DC component is given by \\(X_0\\).\nThe amplitudes of the sinusoids are given by \\(|X_k|\\), and the phases by \\(\\angle X_k\\). Note that for the frequency \\(1/2\\) in particular, (the last term in the sum), the amplitude is \\(X_{N/2}\\), which is a real number, and doesn’t have the factor of 2. Also, its phase is 0, because \\(X_{N/2}\\) is real.\n\n\n\n\n\n\n\nExercises\n\n\n\n\nConsider a periodic signal \\(x[n]\\) with period \\(N=6\\) and the DFT coefficients:\n\\(X_k\\) = [15.0000 + 0.0000i , -2.5000 + 3.4410i , -2.5000 + 0.8123i , -2.5000 - 0.8123i , -2.5000 - 3.4410i]\nWrite \\(x[n]\\) as a sum of sinusoids.\nDo the same for a periodic signal \\(x[n]\\) with period \\(N=5\\) and the DFT coefficients:\n\\(X_k\\) = [21.0000 + 0.0000i , -3.0000 + 5.1962i , -3.0000 + 1.7321i , -3.0000 + 0.0000i , -3.0000 - 1.7321i , -3.0000 - 5.1962i]\nWrite \\(x[n]\\) as a sum of sinusoids.\n\n\n\n\n\n\n2.4.3 The DFT matrix\nA particularly useful way to think about the DFT is as a matrix multiplication. Indeed, applying the DFT to a signal \\(x[n]\\) of length N (i.e. one period) is equivalent to multiplying the signal by a matrix \\(\\mathbf{W}_N\\) of size \\(N \\times N\\): \\[\\mathbf{X} = \\mathbf{W}_N \\cdot \\mathbf{x}\\] i.e.: \\[\n\\begin{bmatrix}\n    X_0 \\\\\n    X_1 \\\\\n    \\vdots \\\\\n    X_{N-1}\n\\end{bmatrix}\n=\n\\underbrace{\n  \\frac{1}{\\sqrt{N}} \\begin{bmatrix}\n  1 & 1 & 1 & \\cdots & 1 \\\\\n  1 & w & w^2 & \\cdots & w^{N-1} \\\\\n  1 & w^2 & w^4 & \\cdots & w^{2(N-1)} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & w^{N-1} & w^{2(N-1)} & \\cdots & w^{(N-1)(N-1)}\n  \\end{bmatrix}\n}_{\\mathbf{W}_N}\n\\cdot\n\\begin{bmatrix}\n    x[0] \\\\\n    x[1] \\\\\n    \\vdots \\\\\n    x[n-1]\n\\end{bmatrix}\n\\] where \\(w = e^{-j \\frac{2\\pi}{N}}\\) is the \\(N\\)-th root of unity.\nThe inverse DFT is also a matrix multiplication, with the inverse matrix \\(\\mathbf{W}_N^{-1}\\) which is the conjugate transpose of \\(\\mathbf{W}_N\\): \\[\\mathbf{x} = \\mathbf{W}_N^{-1} \\cdot \\mathbf{X}\\] with: \\[\\mathbf{W}_N^{-1} = \\mathbf{W}_N^T\\]\nThe DFT matrix \\(\\mathbf{W}_N\\) is a unitary matrix, meaning that its inverse is its conjugate transpose.\nThere might be small variations of the matrix definitions in various sources, depending on whether we have \\(\\frac{1}{\\sqrt{N}}\\) at both DFT and IDFT matrices, or just put \\(\\frac{1}{N}\\) just for the IDFT matrix.\nUnderstanding the DFT and IDFT as mere matrix multiplications with certain fixed \\(N \\times N\\) matrices, mapping a vector of length \\(N\\) to another vector of length \\(N\\), is very useful because it allows us to understand the DFT as a linear operation, and to apply all the properties of linear algebra.\nIn practice, the DFT is computed using the Fast Fourier Transform (FFT) algorithm, which is much faster than the direct matrix multiplication\n\nIn the world of algorithms, the computational complexity of an algorithm is a measure of the amount of resources necessary to run it, most often the number of multiplications. The straightforward matrix multiplication of a vector of size \\(N\\) with the DFT matrix has a computational complexity of \\(\\mathcal{O}(N^2)\\), meaning that the number of multiplications necessary grows quadratically with the size of the signal (note that only the dominant term matters, without coefficient, e.g we write \\(O(N^2)\\) not \\(O(7.3 N^2 + 4N)\\)). This is prohibitively large for large signals.\nThe Fast Fourier Transform (FFT) algorithm exploits the particular nature of the DFT matrix, and reduces the computational complexity to \\(\\mathcal{O}(N \\log_2(N))\\). This is a huge improvement. Consider for example a vector of size \\(N=1024\\). The number of multiplications for the naive matrix multiplication would be of the order of \\(1024^2 = 1,048,576\\), whereas for the FFT algorithm it would be \\(1024 \\log_2(1024) \\approx 1024 \\times 10 = 10,240\\), which is two orders of magnitude smaller (e.g. about 100 times faster).\nThe FFT algorithm is one of the most important algorithms in signal processing. The invention and adoption of the FFT in the 1960s by Cooley and Tukey is considered by many as “the birth of Digital Signal Processing”.\n\n2.4.3.1 Other transforms\nIn the world of discrete signals, there are many signal transforms possible, and many of them can be expressed as matrix multiplications, just like the DFT, but with different matrices.\nIn fact, we can view such a transform as a way of expressing a N-dimensional vector \\(x\\) as a linear combination of a set of \\(N\\) basis vectors:\n\nPut the \\(N\\) vectors of the basis as columns in a matrix (let’s name it \\(\\mathbf{A}\\))\nThe inverse transform is then written as: \\[\\mathbf{x} = \\mathbf{A} \\cdot \\mathbf{X}\\] which means that \\(\\mathbf{x}\\) is a sum of the columns of \\(\\mathbf{A}\\), with coefficients given by \\(\\mathbf{X}\\).\nThe direct transform is then equivalent to finding the coefficients \\(\\mathbf{X}\\): \\[\\mathbf{X} = \\mathbf{A}^{-1} \\cdot \\mathbf{x}\\] which means that \\(\\mathbf{X}\\) is the coefficients of \\(\\mathbf{x}\\) in the basis of \\(\\mathbf{A}\\).\n\nThis is exactly the case of the DFT, where the basis vectors are the complex exponentials \\(e^{j 2 \\pi k n / N}\\), and moreover we also have that the matrix \\(\\mathbf{A}^{-1}\\) is equal to \\(\\mathbf{A}^H\\).\nThe reasons of using transforms, DFT or others, are multiple: compression, denoising, feature extraction, etc. As an example, the discrete cosine transform (DCT) is used in JPEG image compression.\n\n\n\n\n\n\nTransform examples\n\n\n\n\n2.4.3.2 Transform example 1\nConsider the exercise from Week 2:\n\\[x[n] = \\lbrace ..., 0, \\frac{1}{3}, \\frac{2}{3}, 1, 1, 1, 1, 0, ... \\rbrace\\] Write the expression of \\(x[n]\\) based on the signal \\(u[n]\\).\nSolve this in Matlab using a matrix approach.\n\n\n2.4.3.3 Another example: JPEG 2D compression\n\n\n\nJPEG compression with DCT transform1\n\n\nIn JPEG compression, the image is divided into 8x8 blocks, and each block is transformed using the 2D Discrete Cosine Transform (DCT). This exactly corresponds to the matrix multiplication approach we discussed above, except that the basis vectors are the 64 DCT basis vectors.\n\n\n\n8x8 DCT basis vectors2\n\n\nThe DCT transform transforms the 64 pixels of a block into 64 DCT coefficients, similar to the DFT transforming a signal of length \\(N\\) into \\(N\\) DFT coefficients.\nWhy? Because, unlike the 64 pixel values, out of the 64 DCT coefficients many are small negligible, and can be quantized to zero, leading to compression. When the image is reconstructed, the DCT coefficients are transformed back into pixel values using the inverse DCT, and the quantized or missing DCT coefficients not having a large effect on the image quality.\n\n\n\n\n\n\n2.4.4 Properties of the DFT\n\n2.4.4.1 Linearity\nIf the signal \\(x_1[n]\\) has the DFT coefficients \\(X_k^{(1)}\\), and \\(x_2[n]\\) has \\(X_k^{(2)}\\), then their sum has \\[a \\cdot x_1[n] + b\\cdot x_2[n] \\leftrightarrow a \\cdot X_k^{(1)} + b\\cdot X_k^{(2)} \\]\nProof: via definition. Also, since DFT is equivalent to a matrix multiplication, this property is inherited from the linearity of matrix multiplication.\n\n\n2.4.4.2 Shifting in time\nThe DFT coefficients of a signal \\(x[n]\\) delayed by \\(n_0\\) are the DFT coefficients of the original signal, multiplied by a complex exponential: If \\(x[n] \\leftrightarrow  X_k\\), then \\[x[n - n_0] \\leftrightarrow e^{(-j 2 \\pi k n_0 / N)} X_k\\]\nProof: via definition\nNote that \\[ | e^{(-j 2 \\pi k n_0 / N)} X(\\omega) | = | X(\\omega) |\\] which shows that when a signal is shifted in time, the amplitudes \\(|X(\\omega)|\\) of the spectrum are not affected. Shifting in time affects only the phase of the spectrum. This makes sense, because shifting a signal in time does not change the amplitudes of the composing sinusoids, it only shifts them.\n\n\n2.4.4.3 Modulation in time\nA shift of the spectrum \\(X_k\\) by \\(k_0\\) corresponds to a modulation of the signal \\(x[n]\\) by a complex exponential:\n\\[e^{j 2 \\pi k_0 n / N} \\leftrightarrow X_{k-k_0}\\]\n\n\n2.4.4.4 Complex conjugation\nComplex conjugation in time domain corresponds in frequency domain to complex conjugation and inversion of the frequency:\n\\[x^*[n] \\leftrightarrow X_{-k}^*\\]\n\n\n2.4.4.5 Circular convolution\nWe define now the circular convolution of two signals \\(x_1[n]\\) and \\(x_2[n]\\) with period \\(N\\), which is slightly different from the normal convolution.\nCircular convolution is defined as:\n\\[x_1[n] \\otimes x_2[n] = \\sum_{k=0}^{N-1} x_1[k] x_2[(n-k)_N]\\]\nCIrcular convolution is similar to normal convolution, but specifically for periodic signals. Thus, it takes two periodic signals of period N (in fact, two periods of the signals), and the result is also a periodic signal of period N (one period of the result). In other words, it takes two vectors of length N and produces another vector of length N, each of these vectors being a period of a periodic signal of period N. This is different from the normal convolution, which produces a result that is longer than the original signals, i.e. a vector of length 2N.\nTBD: Example at the whiteboard: how it is computed\nThe property of DFT is that the circular, not the linear, convolution of two signals in time domain corresponds to the product of their DFT coefficients in frequency domain: \\[x_1[n] \\otimes x_2[n] \\leftrightarrow N \\cdot X_k^{(1)} \\cdot X_k^{(2)}\\]\n\n\n2.4.4.6 Product in time\nThe product of two signals in time domain corresponds to the circular convolution of their spectra in frequency domain: \\[x_1[n] \\cdot  x_2[n] \\leftrightarrow \\sum_{m=0}^{N-1} X_m^{(1)} X_{(k-m)_N}^{(2)} = X_k^{(1)} \\otimes X_k^{(2)}\\]\n\n\n2.4.4.7 Parseval theorem\nThe Parseval theorem states that the energy of the signal is the same in time and frequency domains: \\[E = \\sum_{0}^{N-1} |x[n]|^2 = \\frac{1}{2 \\pi} \\sum |X_k|^2\\]\nThe energy of a signal is defined as the sum of the squares of its values. Here we have two discrete set of values both in time and frequency domains, so we use the sum instead of the integral in both domains.\n\n\n\n2.4.5 Examples of DTFT and DFT\n\n\n\n\n\n\nExample\n\n\n\nLet’s plot / sketch DTFT and DFT of various signals\nDTFT of:\n\na constant signal \\(x[n] = A\\)\na rectangular signal \\(x[n] = A\\) between \\(-\\tau\\) and \\(\\tau\\), 0 elsewhere\na cosine of frequency precisely \\(f = k/N\\)\na cosine of frequency not \\(f = k/N\\)\n\nDFT, with N=20, of:\n\na constant signal \\(x[n] = A\\)\na rectangular signal \\(x[n] = A\\) between \\(-\\tau\\) and \\(\\tau\\), 0 elsewhere\na cosine of frequency precisely \\(f = k/N\\)\na cosine of frequency not \\(f = k/N\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#relationship-between-dtft-and-dft",
    "href": "04_FourierTransform.html#relationship-between-dtft-and-dft",
    "title": "2  The Fourier Transform",
    "section": "2.5 Relationship between DTFT and DFT",
    "text": "2.5 Relationship between DTFT and DFT\nThe DTFT transform is used for non-periodic signals, and produces a continuous spectrum.\nThe DFT transform is used for periodic signals, and produces a discrete spectrum.\nWe observe the fundamental duality of time and frequency, same for all Fourier transforms: a signal periodic in time has a spectrum discrete in frequency.\n\nDiscrete in one domain \\(\\leftrightarrow\\) Periodic in the other domain\n\ndiscrete in time –&gt; periodic in frequency\nperiodic in time –&gt; discrete in frequency\n\nContinuous in one domain \\(\\leftrightarrow\\) Non-periodic in the other domain\n\ncontinous in time –&gt; non-periodic in frequency\nnon-periodic in time –&gt; continuous in frequency\n\n\nMoreover, there is a close relationship between the two values of the transforms. The discrete DFT coefficients are samples of the continuous DTFT spectrum of a single period of the signal (assuming the signal is surrounded by zeros).\nIn general, consider a non-periodic signal \\(x[n]\\), from which we construct a periodic signal \\(x_N[n]\\) by repeating \\(x[n]\\) every \\(N\\) samples (we “periodize” \\(x[n]\\) with period \\(N\\)).\nThe non-periodic \\(x[n]\\) has a continuous spectrum \\(X(\\omega)\\), and the periodic signal \\(x_N[n]\\) has \\(N\\) discrete DFT coefficients \\(X_k\\). The values of \\(X_k\\) are samples of the continuous spectrum \\(X(\\omega)\\) at frequencies \\(k/N\\): \\[X_k = X(2 \\pi (k/N) n)\\]\nTo illustrate this, consider a sequence of 7 values: \\[x = [6, 3, -4, 2, 0, 1, 2]\\] and let’s compute both the DTFT and DFT starting from these values.\n\nIf we consider a \\(x\\) surrounded by infinitely long zeros, i.e. \\(x[n]\\) is non-periodical, we have a continuous spectrum \\(X(\\omega)\\) given by the DTFT: \\[x_{non-per}[n] = [...0, 0, \\underbrace{6, 3, -4, 2, 0, 1, 2}_{\\textrm{one period}}, 0, 0, ...]  \\leftrightarrow X(\\omega)\\]\nIf we consider that \\(x\\) just one period of a periodic signal, the values repeating themselves every 7 samples: \\[x_{per_7}[n] = \\sum_{k=-\\infty}^{\\infty} x[n - 7 k]\\] we can compute the DFT of this signal, based on the 7 values: \\[x_{per_7}[n] = [..., -4, 2, 0, 1, 2, \\underbrace{6, 3, -4, 2, 0, 1, 2}_{\\textrm{one period}}, 6, 3, -4, ...]  \\leftrightarrow X_k \\]\nThe values \\(X_k\\) are just samples from \\(X(\\omega)\\), at taken at frequencies \\(k/N\\): \\[X_k = X(2 \\pi (k/N) n)\\]\nWe can even repeat the 7 values with a larger period, e.g. \\(N=10\\), by adding zeros in between: \\[x_{per_{10}}[n] = \\sum_{k=-\\infty}^{\\infty} x[n - 10 k]\\] \\[x_{per_{10}}[n] = [..., 0, 6, 3, -4, 2, 0, 1, 2, 0, 0, 0, \\underbrace{6, 3, -4, 2, 0, 1, 2, 0, 0, 0}_{\\textrm{one period}}, 6, 3, -4, ...]  \\leftrightarrow X_k \\]\nThe values \\(X_k\\) are still samples from \\(X(\\omega)\\), but since \\(N=10\\), we have 10 samples taken at frequencies \\(k/10\\):\n\nThe illustration below shows this relationship, for a vector \\(x = [6, 5, 4, -3, 2, -3, 4, 5, 6]\\).:\n\nThe red line is the DTFT of the values \\(x\\), which assumes it is surrounded by zeros and forms a non-periodic signal\nThe blue line is the DFT of the values \\(x\\), which assumes they are periodic. Note that the DFT is just samples from the DTFT.\n\nimport matplotlib.pyplot as plt, numpy as np, scipy as sp\nimport scipy.fftpack\nx = np.array([6, 5, 4, -3, 2, -3, 4, 5, 6])\nN = x.size\nbigN = 1000*N\nn = np.arange(0,N)    # n = [0, 1, 2, ... N-1]\nplt.figure(figsize=(12,6))\nplt.stem(n*1000, np.abs(sp.fftpack.fft(x)))\nplt.plot(np.arange(0,bigN), np.abs(sp.fft.fft(x, bigN)), color='r', linestyle='--')\nplt.savefig('fig/04_RelationshipDTFTDFT.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\nNote that in practice we can’t compute the DTFT of a signal with numerical tools like Matlab, because the DTFT is a continuous function. Instead, we can extend the signal with many zeros, and then compute the DFT of the extended signal. We will have therefore many samples of the continous spectrum, enough to plot it as a continuous function. For example, to plot the continuous spectrum of a signal \\(x\\) above, we extended it with zeros to a length of 10000 samples and then computed the 10000 DFT coefficients, which is achieved by the fft(x, 10000) function call in Matlab. There is no other way to compute the DTFT of a signal with numerical tools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#signal-windowing-and-frequency-resolution",
    "href": "04_FourierTransform.html#signal-windowing-and-frequency-resolution",
    "title": "2  The Fourier Transform",
    "section": "2.6 Signal windowing and frequency resolution",
    "text": "2.6 Signal windowing and frequency resolution\nIn the following, we discuss the effect of working with a finite segment of a signal, which is always the case in practice.\n\n2.6.1 Initial example\nConsider the\nn = 0:99;\nf = 0.015;\nx = cos(2*pi*f*n)\nplot(abs(fft(x)))\nDiscuss:\n\nWhy is the spectrum not just 2 Diracs, like a normal cos()?\nFFT assumes periodicity. Are there boundary problems?\nWhat is the role of the rectangular window?\nWhat happens if we run fft(x, 10000) instead of fft(x)?\n\n\n2.6.1.1 Discussion\nWhen we have the finite-length cosine vector \\(x\\), we have just a part of the signal. The true signal \\(x\\), infinitely long, is actually multiplied with a rectangular window \\(w[n]\\) \\[x = cos(2 \\pi f n) \\cdot w[n]\\].\nMultiplication in time means convolution in frequency, and thus the spectrum of the true infinitely-long \\(x\\), i.e. two Diracs, is convoluted with the spectrum of the rectangular window, \\(W(\\omega)\\).\nThus, the spectrum of our finite-length \\(x\\) is not just two Diracs, but the convolution of two Diracs with \\(W(\\omega)\\). Thus, instead of two Diracs, we have a \\(sinc()\\)-like function, with a wide peak and secondary lobes.\n\n\n\n2.6.2 Signal windowing and frequency resolution\nWorking with a finite-length segment of a signal always distorts its spectrum.\nSuppose we have a very long, possibly infinitely-long, signal \\(x[n]\\) with a spectrum \\(X(\\omega)\\).\nOut of this signal, we take a segment of length \\(N\\), \\(x_w[n]\\) (for example, in order to process it with a computer).\nTaking this segment is equivalent to multiplying the signal with a rectangular window \\(w[n]\\) of length \\(N\\): \\[x_w[n] = x[n] \\cdot w[n]\\] The window \\(w[n]\\) can be the rectangular window, which leaves \\(N\\) samples unchanged and zeroes the rest, or any other window or function.\nThe spectrum of the windowed signal \\(x_w[n]\\) is the convolution of the spectrum of the original signal \\(X(\\omega)\\) with the spectrum of the window \\(W(\\omega)\\): \\[X_w(\\omega) = X(\\omega) * W(\\omega)\\]\nAs a result, the spectrum of the windowed signal \\(x_w[n]\\) is not the same as the spectrum of the original signal \\(x[n]\\).\nIf the true signal \\(x[n]\\) is infinitely long, the spectrum \\(X(\\omega)\\) is composed of just two Diracs. But if we have only a segment of the signal, which is always the case in practice, the spectrum is not just two Diracs, but a convolution of two Diracs with the spectrum of the window \\(W(\\omega)\\). As a result, every Dirac in the spectrum of the original signal is “smudged” into a \\(W(\\omega)\\). This is unavoidable.\nDifferent windows \\(w[n]\\) have different spectra \\(W(\\omega)\\). For example, the rectangular window has a \\(sinc()\\)-like spectrum, with a narrow central peak but large secondary lobes. Other windows, like the Hamming or Hann window, trade the width of the central peak with the height of the secondary lobes. However, every window \\(w[n]\\) will always distort the spectrum of the signal.\nThe only way to mitigate this effect is to work with a longer segment of the signal, which means taking more samples.\nWhat is the problem with this? We lose frequency resolution.\nFrequency resolution is the ability to distinguish between closely spaced frequency components in a signal, e.g. two Diracs close to each other.\nThe window spectrum \\(W(\\omega)\\) “blurs” the spectrum of the signal. We are looking at the spectrum through a “blurred lens” \\(W(\\omega)\\). For example, two close Diracs are be impossible to differentiate, because the “blur” will change them into overlapping peaks, which we can’t separate. Secondary lobes of the window \\(W(\\omega)\\) will mask other Diracs in the spectrum, smaller but further away.\nIn short, working with a finite-length segment of a signal always distorts its spectrum. Having a short segment of a signal leads to low frequency resolution, because the spectrum is “blurred” by the window. The only way to increase frequency resolution is to work with a longer segment of the signal, which means taking more samples. Frequency resolution is proportional to the length of the segment.\nThere are many windows available, each with different spectra \\(W(\\omega)\\).\n\nRecangular window\nHamming window\nHann window\n…\n\nThe rectangular window has the narrowest central peak, but the largest secondary lobes. Other windows have wider central peaks, but smaller secondary lobes. They can be useful to attenuate sharp transitions at the boundaries of the signal, thus reducing the boundary problems.\n\n\n\n\n\n\nRemember\n\n\n\nEvery time we work with a piece of a signal (for example we process an audio file working on segments of 1024 samples), we are applying windowing, and this distorts the spectrum.\nEven the rectangular window is still a window. Always consider if it is useful to replace the rectangular window with another one, especially if you use fft() or other frequency-based operations (hint: most often it is).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#stft-and-spectrograms",
    "href": "04_FourierTransform.html#stft-and-spectrograms",
    "title": "2  The Fourier Transform",
    "section": "2.7 STFT and Spectrograms",
    "text": "2.7 STFT and Spectrograms\nHow can we analyze the frequency of a signal whose frequency components change in time (e.g. like a musical song)?\nThe Short-Time Fourier Transform (STFT) is a a technique for analyzing the frequency content of local sections of a signal as it changes over time.\nSTFT divides a longer time signal into shorter segments of equal length and then computes the Fourier Transform separately on each short segment:\n\nSplit the signal into pieces (e.g. 1024-samples long), possibly overlapping\nCompute the spectrum of every piece (e.g. fft())\nDisplay the resulting sequence of spectra as an image, known as a spectrogram.\n\nExamples: https://en.wikipedia.org/wiki/Spectrogram\nThe STFT is a time-frequency representation of a signal, because the resulting spectrogram is 2D, with two axes: time and frequency.\nIt is common for the segments to overlap, e.g. 10% overlap, in order to have a smoother transition between segments.\nThe STFT depends on two parameters:\n\nThe length of the segments (e.g. 1024 samples)\nThe overlap between segments (e.g. 10%)\n\nThe length of the segments affects the time resolution and the frequency resolution of the spectrogram.\n\nIf the segments have short length, we have good time resolution, because we can pinpoint the moment in time where a frequency component appears with high precision. However, we have poor frequency resolution, because the spectrum of a short segment is more “blurred”\nIf the segments have long length, we have poor time resolution, because we can’t pinpoint the moment in time where a frequency component appears with high precision, but we have good frequency resolution, because the spectrum of a long segment is less “blurred”.\n\nThis illustrates the Time-frequency Uncertainty Principle, which states that you cannot have very good time resolution and very good frequency resolution simultaneously. This principle is often encountered in many areas of engineering, physics, or mathematics.\nA small overlap between segments is useful to have a smoother transition between segments, and avoid hard-splitting a frequency component between two adjacent segments. Common values range from 10% to 50%.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#geometric-interpretation-of-fourier-transform-dtft",
    "href": "04_FourierTransform.html#geometric-interpretation-of-fourier-transform-dtft",
    "title": "2  The Fourier Transform",
    "section": "2.8 Geometric interpretation of Fourier Transform (DTFT)",
    "text": "2.8 Geometric interpretation of Fourier Transform (DTFT)\nConsider a signal \\(x[n]\\) with the Z-transform \\(X(z)\\), with zeros \\(z_1, z_2, \\dots\\) and poles \\(p_1, p_2, \\dots\\): \\[X(z) = C \\cdot \\frac{(z-z_1)\\cdots(z - z_M)}{(z-p_1)\\cdots(z - p_N)}\\]\nThe DTFT is: \\[X(\\omega) = C \\cdot \\frac{(e^{j \\omega}-z_1)\\cdots(e^{j \\omega} - z_M)}{(e^{j \\omega}-p_1)\\cdots(e^{j \\omega} - p_N)}\\] with its modulus function being: \\[|X(\\omega)| = |C| \\cdot \\frac{|e^{j \\omega}-z_1|\\cdots|e^{j \\omega} - z_M|}{|e^{j \\omega}-p_1|\\cdots|e^{j \\omega} - p_N|}\\] and phase: \\[\\angle{X} = \\angle{C} + \\angle (e^{j \\omega}-z_1) + \\cdots + \\angle(e^{j \\omega} - z_M) - \\angle(e^{j \\omega}-p_1) - \\cdots - \\angle(e^{j \\omega} - p_N)\\]\nFor any complex numbers \\(a\\) and \\(b\\), we have:\n\nmodulus of \\(|a - b|\\) is the length of the segment between \\(a\\) and \\(b\\)\nphase of \\(|a - b|\\) is the angle of the segment from \\(b\\) to \\(a\\) (direction is important)\n\nSo, for a point on the unit circle \\(z = e^{j \\omega}\\), we have:\n\nthe modulus \\(|e^{j \\omega} - z_k|\\) or \\(|e^{j \\omega} - p_k|\\) is the distance of that point from the zero or pole \\(z_k\\) or \\(p_k\\)\nphase of \\(|e^{j \\omega} - z_k|\\) or \\(|e^{j \\omega} - p_k|\\) is the angle of the segment from the zero or pole to that point\n\nAs such, we can say that the modulus of the DTFT \\(|X(\\omega)|\\), for a certain value of \\(\\omega\\), is given by the distances to the zeros and to the poles, and the phase of the DTFT \\(\\angle{X(\\omega)}\\) is given by the angles of segments from the zeros and poles to that point.\nTBD: Add figures\nConsequences:\n\nwhen a pole is very close to unit circle, the DTFT is large around this point (around that value of \\(\\omega\\))\nwhen a zero is very close to unit circle, the DTFT is small around this point (around that value of \\(\\omega\\))\n\nExamples: …\nThis geometric interpretation ignores the constant \\(C\\) in front of the Z-transform, which is not visible in the pole-zero plot. The value of \\(|C|\\) and \\(\\angle{C}\\) must be determined separately, from another information (for example, the value of the transform at a certain point).\n\n\n\n\n\n\nIntuition\n\n\n\nWe can think of the Z transform \\(X(z)\\) as a landscape, with poles being mountains (of infinite height) and zeros being valleys (of zero height). and zeros being valleys.\nThe DTFT \\(X(\\omega)\\) is like a walk over this landscape along the unit circle. The height profile of the walk gives the modulus of the Fourier transform.\n\nWhen close to a mountain, the road is high, and the Fourier transform has large amplitude\nWhen close to a valley, the road is low, and the Fourier transform has small amplitude\n\nThis allows to quickly understand and possibly sketch the shape of the Fourier Transform of a signal, just by looking at the pole-zero plot.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#terminology",
    "href": "04_FourierTransform.html#terminology",
    "title": "2  The Fourier Transform",
    "section": "2.9 Terminology",
    "text": "2.9 Terminology\nBased on the frequency content of signals, we can classify them in various categories:\n\nlow-frequency signals\nmid-frequency signals\nhigh-frequency signals\n\nBand-limited signals are signals that have all the spectrum within a certain frequency band. e.g. no larger than a frequency \\(f_{max}\\), and zero elsewhere.\nThe bandwitdh \\(B\\) of a signal is the frequency interval [\\(f_1\\), \\(f_2\\)] which contains \\(95\\%\\) of energy. The size of the bandwidth is \\(B = f_2 - f_1\\).\nBased on bandwidth \\(B\\), we can define the central frequency \\(F_c = \\frac{f_1 + f_2}{2}\\), and classify signals in:\n\nNarrow-band signals: their bandwidth \\(B\\) is much smaller than the central frequency \\(F_c\\) \\[B &lt;&lt; F_c\\]\nWide-band signals: signals which are not narrow-band",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "04_FourierTransform.html#footnotes",
    "href": "04_FourierTransform.html#footnotes",
    "title": "2  The Fourier Transform",
    "section": "",
    "text": "image from: JPEG Picture Compression Using Discrete Cosine Transform, N. K. More, S. Dubey, 2012↩︎\nimage from Wikipedia↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Fourier Transform</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Ex01_Sampling.html",
    "href": "Ex01_Sampling.html",
    "title": "3  Sampling",
    "section": "",
    "text": "3.1 Exercise 1\nAre the following signals periodic? If yes, compute their period.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "Ex01_Sampling.html#exercise-1",
    "href": "Ex01_Sampling.html#exercise-1",
    "title": "3  Sampling",
    "section": "",
    "text": "a). \\(x[n] = \\cos(\\pi \\frac{3}{10} n)\\)\nb). \\(x[n] = \\cos(7.2 \\pi n)\\)\nc). \\(x[n] = \\sin(3n)\\)\nd). \\(x[n] = \\sin(\\frac{\\pi n}{2}) + \\cos(\\frac{3 \\pi n}{4})\\)\n\n\nSolution\na). We want to find the smallest positive integer \\(N\\) such that \\(x[n] = x[n+N]\\), i.e. \\[\\begin{aligned}\n\\cos(\\pi \\frac{3}{10} n) &= \\cos(\\pi \\frac{3}{10} (n+N)) \\\\\n&= \\cos(\\pi \\frac{3}{10} n + \\pi \\frac{3}{10} N) \\\\\n\\end{aligned}\\]\nThis is true if the extraterm \\(\\pi \\frac{3}{10} N\\) is \\(2 \\pi k\\), i.e.: \\[\\begin{aligned}\n\\pi \\frac{3}{10} N &= 2 \\pi k \\\\\n\\frac{3}{10} N &= 2 k \\\\\nN &= \\frac{20}{3} k\n\\end{aligned}\\] with \\(k\\) integer. The smallest positive integer \\(N\\) is \\(N=20\\) (when \\(k=3\\)). Therefore the signal is periodic with period \\(N=20\\).\nb). We want to find the smallest positive integer \\(N\\) such that \\(x[n] = x[n+N]\\), i.e. \\[\\begin{aligned}\n\\cos(7.2 \\pi n) &= \\cos(7.2 \\pi (n+N)) \\\\\n&= \\cos(7.2 \\pi n + 7.2 \\pi N) \\\\\n\\end{aligned}\\]\nThis is true if \\(7.2 \\pi N = 2 \\pi k\\), i.e.: \\[\\begin{aligned}\n7.2 \\pi N &= 2 \\pi k \\\\\n3.6 N &= k \\\\\n\\end{aligned}\\] with \\(k\\) integer. The smallest positive integer \\(N\\) is \\(N=5\\).\nAn alternative way is to to treat this as a continuous signal and identify the frequency of the cosine: \\[\\begin{aligned}\n\\cos(7.2 \\pi n) &= \\cos(2 \\pi f n) \\\\\nf &= 3.6Hz\n\\end{aligned}\\] The period should be \\(N=\\frac{1}{f} = \\frac{1}{3.6} = \\frac{5}{18}\\), but since we need \\(N\\) to be an integer because our signal ia actually discrete, we consider the smallest integer multiple of this, which is: \\[N = \\frac{5}{18} \\cdot 18 = 5\\] Therefore the signal is periodic with period \\(N=5\\).\nc). We want to find the smallest positive integer \\(N\\) such that \\(x[n] = x[n+N]\\), i.e. \\[\\begin{aligned}\n\\sin(3 n) &= \\sin(3 (n+N)) \\\\\n&= \\sin(3 n + 3 N) \\\\\n\\end{aligned}\\]\nThis is true if \\(3 N = 2 \\pi k\\), i.e.: \\[\\begin{aligned}\n3 N &= 2 \\pi k \\\\\nN &= \\frac{2}{3} \\pi k \\\\\n\\end{aligned}\\] with \\(k\\) integer.\nThis is impossible, because \\(k\\) is an irrational number which means that multiplying and dividing it by the integer numbers 2, 3, \\(k\\) will never become an integer as we need \\(N\\) to be.\nTherefore the signal is not periodic.\nd). We want to find the smallest positive integer \\(N\\) such that \\(x[n] = x[n+N]\\), i.e. \\[\\begin{aligned}\n\\sin(\\frac{\\pi n}{2}) + \\cos(\\frac{3 \\pi n}{4}) &= \\sin(\\frac{\\pi}{2} (n+N)) + \\cos(\\frac{3 \\pi}{4} (n+N)) \\\\\n&= \\sin(\\frac{\\pi}{2} n + \\frac{\\pi}{2} N) + \\cos(\\frac{3 \\pi}{4} n + \\frac{3 \\pi}{4} N) \\\\\n\\end{aligned}\\]\nThis is true if \\(\\frac{\\pi}{2} N = 2 \\pi k\\) and \\(\\frac{3 \\pi}{4} N = 2 \\pi k'\\), i.e.: \\[\\begin{aligned}\n\\frac{\\pi}{2} N &= 2 \\pi k \\\\\n\\frac{3 \\pi}{4} N &= 2 \\pi k' \\\\\n\\end{aligned}\\] with \\(k\\), \\(k'\\) being integers.\nThis means: \\[\\begin{aligned}\nN &= 4 k \\\\\nN &= \\frac{8}{3} k' \\\\\n\\end{aligned}\\]\nThe smallest positive integer \\(N\\) is \\(N=8\\) (when \\(k=2\\) and \\(k'=3\\)). Therefore the signal is periodic with period \\(N=8\\).",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "Ex01_Sampling.html#exercise-2",
    "href": "Ex01_Sampling.html#exercise-2",
    "title": "3  Sampling",
    "section": "3.2 Exercise 2",
    "text": "3.2 Exercise 2\nConsider the following signal: \\[x_a(t) = (1 + 0.5 cos(400 \\pi t)) \\cdot cos(8000 \\pi t)\\]\n\na). Compute the minimum sampling frequency necessary for avoiding alias;\nb). The signal is sampled with 8000Hz. Write the discrete signal obtained via sampling;\nc). Does alias occur? If yes, identify the frequencies in the signal which are aliased;\nd). What is the analog signal reconstructed from the samples via ideal D/A.reconstruction?\n\n\nSolution\na). Compute the minimum sampling frequency necessary for avoiding alias;\nThe minimum sampling frequency necessary for avoiding alias is the double of the maximum frequency in the signal, so we need to identify the maximum frequency in the signal.\nFor this we need to convert the multiplication of the two cosines into a sum of cosines, because a multiplication of two cosines produces a frequency which is higher than the starting frequencies.\nUsing the trigonometric identity: \\[\\begin{aligned}\n\\cos(\\alpha) \\cos(\\beta) &= \\frac{1}{2} \\left( \\cos(\\alpha - \\beta) + \\cos(\\alpha + \\beta) \\right)\n\\end{aligned}\\] we have: \\[\\begin{aligned}\nx_a(t) &= (1 + 0.5 \\cos(400 \\pi t)) \\cdot \\cos(8000 \\pi t) \\\\\n&=\\cos(8000 \\pi t) + 0.5 \\cos(8000 \\pi t) \\cos(400 \\pi t) \\\\\n&=\\cos(8000 \\pi t) + 0.5 \\frac{1}{2} \\left( \\cos(8000 \\pi t - 400 \\pi t) + \\cos(8000 \\pi t + 400 \\pi t) \\right) \\\\\n&=\\cos(8000 \\pi t) + 0.25 \\cos(7600 \\pi t) + 0.25 \\cos(8400 \\pi t) \\\\\n\\end{aligned}\\]\n\n\n\n\n\n\nFrequent error\n\n\n\nIf the signal is already given as a sum of sinusoidal components, we don’t need to do anything else, just leave it as it is.\n\n\nNext, we identify the frequencies in the signal, which are: \\[\\begin{aligned}\nf_1 &= 4000Hz \\\\\nf_2 &= 3800Hz \\\\\nf_3 &= 4200Hz \\\\\n\\end{aligned}\\]\n\n\n\nFrequencies in the signal\n\n\nThe minimum sampling frequency for avoiding alias is therefore twice the maximum frequency, which is \\(f_3 = 4200Hz\\), i.e.: \\[F_{s_{min}} = 2 \\cdot 4200Hz = 8400Hz\\]\nb). The signal is sampled with 8000Hz. Write the discrete signal obtained via sampling;\nSampling the signal amounts to applying the variable transformation \\(t = \\frac{n}{F_s}\\), i.e.: \\[\\begin{aligned}\nx[n] &= x_a(t) \\Big|_{t = \\frac{n}{F_s}} \\\\\n&= \\cos(8000 \\pi \\frac{n}{F_s}) + 0.25 \\cos(7600 \\pi \\frac{n}{F_s}) + 0.25 \\cos(8400 \\pi \\frac{n}{F_s}) \\\\\n&= \\cos(8000 \\pi \\frac{n}{8000}) + 0.25 \\cos(7600 \\pi \\frac{n}{8000}) + 0.25 \\cos(8400 \\pi \\frac{n}{8000}) \\\\\n&= \\cos(\\pi n) + 0.25 \\cos(\\frac{19}{20} \\pi n) + 0.25 \\cos(\\frac{21}{20} \\pi n) \\\\\n\\end{aligned}\\]\nc). Does alias occur? If yes, identify the frequencies in the signal which are aliased;\nThe sampling frequency is \\(F_s = 8000Hz\\), which is lower than required for avoiding alias of the third component. Therefore we expect the third component to be aliased, while the first and second, for which \\(F_s\\) is more than double, are not aliased.\nIndeed, let’s identify the frequencies in the discrete signal obtained: \\[\\begin{aligned}\n\\pi n = 2 \\pi f_1 n \\Rightarrow f_1 &= \\frac{1}{2} = 0.5 \\\\\n\\frac{19}{20} \\pi n = 2 \\pi f_2 n \\Rightarrow f_2 &= \\frac{19}{40} = 0.475 \\\\\n\\frac{21}{20} \\pi n = 2 \\pi f_3 n \\Rightarrow f_3 &= \\frac{21}{40} = 0.525 \\\\\n\\end{aligned}\\]\nWe have \\(f_1 &lt; 0.5\\), \\(f_2 &lt; 0.5\\), which means they are not aliased, and \\(f_3 &gt; 0.5\\), which is aliased. The frequency \\(f_3\\) is aliased to: \\[f_3' = f_3 - 1 = 0.525 - 1 = -0.475 = -\\frac{19}{40}\\] which means that: \\[\\cos(2 \\pi \\frac{21}{40} n) = \\cos(- 2 \\pi \\frac{19}{40} n), \\forall n\\]\nd). What is the analog signal reconstructed from the samples via ideal D/A reconstruction?\nIdeal A/D reconstruction means that we need to apply the inverse variable transformation \\(n = t \\cdot F_s\\), but starting from the aliased frequencies in the signal, if any.\nWith the aliased frequency \\(f_3'\\), the discrete signal is: \\[x[n] = \\cos(\\pi n) + 0.25 \\cos(\\frac{19}{20} \\pi n) + 0.25 \\cos(-\\frac{19}{20} \\pi n)\\]\nThe analog signal reconstructed via ideal D/A reconstruction is: \\[\\begin{aligned}\nx_r(t) &= \\cos(\\pi \\cdot t \\cdot F_s) + 0.25 \\cos(\\frac{19}{20} \\pi \\cdot t \\cdot F_s) + 0.25 \\cos(-\\frac{19}{20} \\pi \\cdot t \\cdot F_s) \\\\\n&= \\cos(8000 \\pi t) + 0.25 \\cos(7600 \\pi t) + 0.25 \\cos(- 7600 \\pi t) \\\\\n&= \\cos(8000 \\pi t) + 0.25 \\cos(7600 \\pi t) + 0.25 \\cos(7600 \\pi t) \\\\\n&= \\cos(8000 \\pi t) + 0.5 \\cos(7600 \\pi t)\n\\end{aligned}\\]\nDue to aliasing, the reconstructed signal is not the same as the original signal.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html",
    "href": "Ex02_Systems.html",
    "title": "4  Signals and Systems",
    "section": "",
    "text": "4.1 Exercise 1\nConsider the following discrete signal \\(x[n]\\): \\[x[n] =\n\\begin{cases}\n1 + \\frac{n}{3}, &-3 \\leq n \\leq -1\\\\\n1, &0 \\leq n \\leq 3\\\\\n0, &\\text{elsewhere}\n\\end{cases}\n\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html#exercise-1",
    "href": "Ex02_Systems.html#exercise-1",
    "title": "4  Signals and Systems",
    "section": "",
    "text": "Find the values of \\(x[n]\\) and represent the signal graphically\n\n\nRepresent graphically the signal \\(x[-n + 4]\\)\n\n\nWrite the expression of \\(x[n]\\) based on the signal \\(\\delta[n]\\)\n\n\nWrite the expression of \\(x[n]\\) based on the signal \\(u[n]\\)\n\n\n\nSolution\na). Find the values of \\(x[n]\\) and represent the signal graphically\nGiving values for \\(n\\) in the range \\(-3 \\leq n \\leq 3\\), we obtain the following values for \\(x[n]\\): \\[x[n] = \\{..., 0, \\frac{1}{3}, \\frac{2}{3}, \\underuparrow{1}, 1, 1, 1, 0, ...\\}\\] The graphical representation of the signal is:\n\nb). Represent graphically the signal \\(x[-n + 4]\\)\nLet’s give a name to the signal \\(x[-n + 4]\\), say \\(a[n]\\). We have: \\[a[n] = x[-n + 4]\\]\nTo understand visually what this means, let’s compute a few values of \\(a[n]\\): \\[\\begin{aligned}\na[0]=x[-0+4] &= x[4] = 0 \\\\\na[1]=x[-1+4] &= x[3] = 1 \\\\\na[2]=x[-2+4] &= x[2] = 1 \\\\\n\\end{aligned}\\]\nWe have \\(x[4]\\) moving to \\(a[0]\\), \\(x[3]\\) moving to \\(a[1]\\), \\(x[2]\\) moving to \\(a[2]\\), which means that the signal \\(x[n]\\) is reversed and then shifted.\nWe can continue and extend the signal \\(a[n]\\) to the left and to the right, obtaining the full signal \\(a[n]\\): \\[a[n] = \\{..., \\underuparrow{0}, 1, 1, 1, 1, \\frac{2}{3}, \\frac{1}{3}, 0, ...\\}\\]\n\n\n\nThe signal \\(a[n] = x[-n + 4]\\)\n\n\nc). Write the expression of \\(x[n]\\) based on the signal \\(\\delta[n]\\)\nEach Dirac in the graphical representation of \\(x[n]\\), taken separately, is a shifted impulse, having a certain shift and a certain amplitude.\nEach Dirac line located at position \\(k\\) with amplitude \\(A\\) can be written as \\(A \\cdot \\delta[n - k]\\). For example, the Dirac line located at position \\(-2\\) with amplitude \\(\\frac{1}{3}\\) can be written as the signal \\(\\frac{1}{3} \\cdot \\delta[n + 2]\\).\n\nTherefore, we can write any signal as as sum of impulse signals. In this case, we have: \\[x[n] = \\frac{1}{3} \\delta[n + 2] + \\frac{2}{3} \\delta[n + 1] + \\delta[n] + \\delta[n - 1] + \\delta[n - 2] + \\delta[n - 3]\\]\nd). Write the expression of \\(x[n]\\) based on the signal \\(u[n]\\)\nThere are two ways to write the signal \\(x[n]\\) based on the signal \\(u[n]\\).\nIn the first method, note that we can write any impulse signal as a difference of two step signals, the second one being one step delayed with resoect to the first. For example, the signal \\(\\delta[n]\\) can be written as \\(u[n] - u[n-1]\\), as illustrated in the following figure:\n\n\n\nThe signal \\(\\delta[n]\\) as a difference of two step signals\n\n\nTherefore we can start from the sum of impulses and write each impulse as a difference of two step signals: \\[\\begin{aligned}\nx[n] &= \\frac{1}{3} \\delta[n + 2] + \\frac{2}{3} \\delta[n + 1] + \\delta[n] + \\delta[n - 1] + \\delta[n - 2] + \\delta[n - 3] \\\\\n&= \\frac{1}{3} (u[n + 2] - u[n + 1]) + \\frac{2}{3} (u[n + 1] - u[n]) + (u[n] - u[n - 1]) + (u[n - 1] - u[n - 2]) + (u[n - 2] - u[n - 3]) + (u[n - 3] - u[n-4]) \\\\\n&= \\frac{1}{3} u[n + 2] + \\frac{1}{3} u[n + 1] + \\frac{1}{3}u[n] - u[n - 4]\n\\end{aligned}\\]\nA second, more visually intuitive way, relies on visually decomposing the signal \\(x[n]\\) into a sum of step signals. Observe the figure below and how we can decompose the signal \\(x[n]\\) as a staircase with several steps:\n\n\n\nThe signal \\(x[n]\\) as a sum of step signals\n\n\n\nThe first step starts at \\(n = -2\\) and has a height of \\(\\frac{1}{3}\\), so it can be written as \\(\\frac{1}{3} u[n + 2]\\)\nOn top of this we have a second step starting at \\(n = -1\\) and having a height of \\(\\frac{1}{3}\\), so it can be written as \\(\\frac{1}{3} u[n + 1]\\)\nOn top of these we have a third step starting at \\(n = 0\\) and having a height of \\(\\frac{1}{3}\\), so it can be written as \\(\\frac{1}{3} u[n]\\)\nOn top of these we have a fourth negative step starting at \\(n = 4\\) and having a height of \\(-1\\), so it can be written as \\(-u[n - 4]\\). This one will cancel all the first three, from this point onwards.\n\nThe result can therefore be written as: \\[x[n] = \\frac{1}{3} u[n + 2] + \\frac{1}{3} u[n + 1] + \\frac{1}{3}u[n] - u[n - 4]\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html#exercise-2",
    "href": "Ex02_Systems.html#exercise-2",
    "title": "4  Signals and Systems",
    "section": "4.2 Exercise 2",
    "text": "4.2 Exercise 2\nConsider the following signal: \\[x[n] =\n\\begin{cases}\n1, &-1 \\leq n \\leq 2\\\\\n\\frac{1}{2}, &3 \\leq n \\leq 4\\\\\n0, &elsewhere\n\\end{cases}\n\\]\nRepresent graphically the following signals:\n\na). \\(x[n-2]\\)\nb). \\(x[n+2]\\)\nc). \\(x[4-n]\\)\nd). \\(x[n] \\cdot u[2-n]\\)\ne). \\(x[n-1] \\cdot \\delta[n-3]\\)\nf). \\(x[n^2]\\)\ng). The even part of \\(x[n]\\)\nh). The odd part of \\(x[n]\\)\n\n\nSolution\nThat’s way too much drawing for me, so I’ll just write in words what needs to be done.\na). \\(x[n-2]\\)\nThis is the signal \\(x[n]\\) shifted to the right by 2 steps (i.e. shifted to the future).\nb). \\(x[n+2]\\)\nThis is the signal \\(x[n]\\) shifted to the left by 2 steps (i.e. shifted to the past).\nc). \\(x[4-n]\\)\nDo this just like in Exercise 1.\nd). \\(x[n] \\cdot u[2-n]\\)\nThis is the signal \\(x[n]\\) multiplied by the signal \\(u[2-n]\\).\nDraw first the signal \\(u[2-n]\\), like in Exercise 1. This is a step signal (\\(u[n]\\)) that starts at \\(n = 2\\) and goes towards the left with 1’s (is reversed), and has 0’s towards the right.\nThen, multiply the two signals, point by point. The right part of the signal \\(x[n]\\) will be multiplied by 0’s, so it will vanish, while the left part of the signal \\(x[n]\\) will be multiplied by 1’s, so it will remain.\ne). \\(x[n-1] \\cdot \\delta[n-3]\\)\nThis is the signal \\(x[n-1]\\) multiplied by the signal \\(\\delta[n-3]\\).\nThe signal \\(\\delta[n-3]\\) is a shifted impulse, located at \\(n = 3\\). Therefore it has a single value of 1 at position \\(n = 3\\), and 0’s everywhere else.\nMultiplication with \\(x[n-1]\\) will produce a signal that is 0 everywhere, except at position \\(n = 3\\), where it will have the value of \\(x[n-1]\\) at that position, which should be \\(x[2] = 1\\).\nf). \\(x[n^2]\\)\nGive this signal a new name and compute its values for all \\(n\\), like in Exercise 1. \\[a[n] = x[n^2]\\] \\[a[0] = x[0^2] = x[0] = 1\\] \\[a[1] = x[1^2] = x[1] = 1\\] \\[a[-1] = x[(-1)^2] = x[1] = 1\\] \\[...\\]\ng). The even part of \\(x[n]\\)\nThe even part of a signal is defined as (see Lectures): \\[x_e[n] = \\frac{1}{2} (x[n] + x[-n])\\]\nThe signal \\(x[-n]\\) is the signal \\(x[n]\\) reversed (horizontal flip). Compute the values of \\(x_e[n]\\) for all \\(n\\), one by one, according to this definition.\nh). The odd part of \\(x[n]\\)\nThe odd part of a signal is defined as (see Lectures): \\[x_o[n] = \\frac{1}{2} (x[n] - x[-n])\\]\nThe signal \\(x[-n]\\) is the signal \\(x[n]\\) reversed (horizontal flip). Compute the values of \\(x_o[n]\\) for all \\(n\\), one by one, according to this definition.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html#exercise-3",
    "href": "Ex02_Systems.html#exercise-3",
    "title": "4  Signals and Systems",
    "section": "4.3 Exercise 3",
    "text": "4.3 Exercise 3\nCharacterize the following systems with respect to:\n\nMemory\nLinearity\nTime invariance\nCausality\nStability\n\nThe systems are:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nb). \\(y[n] = x[n] \\cdot cos(\\omega_0 n)\\)\nc). \\(y[n] = \\sin(x[n])\\)\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\n\n\nSolution\nLet us consider each property in turn.\n\nMemory\nA system is memoryless if the output at time \\(n\\) depends only on the input at time \\(n\\). This means \\(y[n]\\) depends only on \\(x[n]\\), for any \\(n\\), without any delays (no \\(x[n-1]\\), \\(x[n+1]\\) etc).\nOtherwise, the system has memory.\nLet’s see the systems one by one:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nHas memory, because, for example, \\(y[2] = 2 x[4]\\), so it depends on \\(x[4]\\), not just on \\(x[2]\\).\nb). \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\)\nIs memoryless, because \\(y[2] = x[2] \\cdot \\cos(\\omega_0 \\cdot 2)\\), \\(y[3] = x[3] \\cdot \\cos(\\omega_0 \\cdot 3)\\), etc. so every \\(y[n]\\) depends only on \\(x[n]\\) and never on \\(x[n-1]\\), \\(x[n+1]\\) etc.\nc). \\(y[n] = \\sin(x[n])\\)\nIs memoryless, because \\(y[2] = \\sin(x[2])\\), \\(y[3] = \\sin(x[3])\\), etc. so every \\(y[n]\\) depends only on \\(x[n]\\) and never on \\(x[n-1]\\), \\(x[n+1]\\) etc.\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\nHas memory because \\(y[n]\\) depends on \\(x[n+1]\\), according to the second term.\n\n\n\nLinearity\nA system is linear if it satisfies the superposition principle, i.e. if the output of the sum of two inputs is equal to the sum of the outputs of the two inputs taken separately. \\[\\mathcal{H}\\left\\{a_1 x_1[n] + a_2 x_2[n]\\right\\} = a_1 \\mathcal{H}\\left\\{x_1[n]\\right\\} + a_2 \\mathcal{H}\\left\\{x_2[n]\\right\\}\\]\nThis means that if we replace \\(x[n]\\) with \\(a_1 x_1[n] + a_2 x_2[n]\\) in the equation, we obtain the same result as if we would have applied the system to \\(x_1[n]\\) and \\(x_2[n]\\) separately and then added the results.\nLet’s see the systems one by one:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nIs linear, because:\n\nwith \\(x_1[n]\\) we have \\(\\mathcal{H}\\left\\{x_1[n]\\right\\} = n \\cdot x_1[n^2]\\)\nwith \\(x_2[n]\\) we have \\(\\mathcal{H}\\left\\{x_2[n]\\right\\} = n \\cdot x_2[n^2]\\)\nwith \\(a x_1[n] + b x_2[n]\\) we have:\n\\[\\begin{aligned}\n\\mathcal{H}\\left\\{a x_1[n] + b x_2[n]\\right\\} &=\nn \\cdot (a x_1[n^2] + b x_2[n^2]) \\\\\n&= a n \\cdot x_1[n^2] + b n \\cdot x_2[n^2] \\\\\n&= a \\mathcal{H}\\left\\{x_1[n]\\right\\} + b \\mathcal{H}\\left\\{x_2[n]\\right\\}\n\\end{aligned}\\]\nNote that it’s not the signal \\(x[n]\\) which is raised to power, but the index \\(n\\). The signal \\(x[n]\\) does therefore not undergo any nonlinear operation.\n\nb). \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\)\nIs linear, because:\n\nwith \\(x_1[n]\\) we have \\(\\mathcal{H}\\left\\{x_1[n]\\right\\} = x_1[n] \\cdot \\cos(\\omega_0 n)\\)\nwith \\(x_2[n]\\) we have \\(\\mathcal{H}\\left\\{x_2[n]\\right\\} = x_2[n] \\cdot \\cos(\\omega_0 n)\\)\nwith \\(a x_1[n] + b x_2[n]\\) we have:\n\\[\\begin{aligned}\n\\mathcal{H}\\left\\{a x_1[n] + b x_2[n]\\right\\} &=\n(a x_1[n] + b x_2[n]) \\cdot \\cos(\\omega_0 n) \\\\\n&= a x_1[n] \\cdot \\cos(\\omega_0 n) + b x_2[n] \\cdot \\cos(\\omega_0 n) \\\\\n&= a \\mathcal{H}\\left\\{x_1[n]\\right\\} + b \\mathcal{H}\\left\\{x_2[n]\\right\\}\n\\end{aligned}\\]\n\nc). \\(y[n] = \\sin(x[n])\\)\nIs not linear, because:\n\nwith \\(x_1[n]\\) we have \\(\\mathcal{H}\\left\\{x_1[n]\\right\\} = \\sin(x_1[n])\\)\nwith \\(x_2[n]\\) we have \\(\\mathcal{H}\\left\\{x_2[n]\\right\\} = \\sin(x_2[n])\\)\nwith \\(a x_1[n] + b x_2[n]\\) we have:\n\\[\\begin{aligned}\n\\mathcal{H}\\left\\{a x_1[n] + b x_2[n]\\right\\} &=\n\\sin(a x_1[n] + b x_2[n]) \\\\\n&\\neq a \\sin(x_1[n]) + b \\sin(x_2[n]) \\\\\n&= a \\mathcal{H}\\left\\{x_1[n]\\right\\} + b \\mathcal{H}\\left\\{x_2[n]\\right\\}\n\\end{aligned}\\]\nThis is not equal to the sum of the previous two.\n\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\nIs linear, because:\n\nwith \\(x_1[n]\\) we have \\(\\mathcal{H}\\left\\{x_1[n]\\right\\} = x_1[n] + n \\cdot x_1[n+1]\\)\nwith \\(x_2[n]\\) we have \\(\\mathcal{H}\\left\\{x_2[n]\\right\\} = x_2[n] + n \\cdot x_2[n+1]\\)\nwith \\(a x_1[n] + b x_2[n]\\) we have:\n\\[\\begin{aligned}\n\\mathcal{H}\\left\\{a x_1[n] + b x_2[n]\\right\\} &=\n(a x_1[n] + b x_2[n]) + n \\cdot (a x_1[n+1] + b x_2[n+1]) \\\\\n&= a x_1[n] + a n \\cdot x_1[n+1] + b x_2[n] + b n \\cdot x_2[n+1] \\\\\n&= a \\mathcal{H}\\left\\{x_1[n]\\right\\} + b \\mathcal{H}\\left\\{x_2[n]\\right\\}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nQuick way of checking linearity\n\n\n\nTo be linear, any signal which appears in the equation (\\(x[n]\\), \\(x[n-1]\\), \\(y[n]\\) etc.) should not undergo any nonlinear operation, such as raising to power, trigonometric functions etc. Any such operation will make the system nonlinear, because for example: \\[sin(x_1[n] + x_2[n]) \\neq \\sin(x_1[n]) + \\sin(x_2[n])\\] \\[(x_1[n] + x_2[n])^2 \\neq x_1[n]^2 + x_2[n]^2\\] \\[...\\]\nThe only operations allowed which preserve linearity are:\n\nmultiplication by something which is not a signal (i.e. \\(x[n] \\cdot x[n+1]\\) is not linear, but \\(x[n] \\cdot 3\\) is linear, \\(x[n] \\cdot \\cos(\\omega_0 n)\\) is linear)\ndelay (shift) by a constant, i.e. \\(x[n+3]\\), \\(y[n-1]\\) etc.\nsums between the above\n\n\n\n\n\nTime invariance\nA system is time-invariant if a time shift in the input signal, \\(x[n-k]\\), produces a corresponding time shift in the output signal, but no other change, i.e.: \\[\\mathcal{H}\\left\\{x[n - k]\\right\\} = y[n - k]\\]\nLet’s see the systems one by one:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nIs not time-invariant, because:\n\nwith \\(x[n-k]\\) we have \\(\\mathcal{H}\\left\\{x[n-k]\\right\\} = n \\cdot x[(n-k)^2]\\)\ndelaying the output by \\(k\\) we have \\(y[n-k] = (n-k) \\cdot x[(n-k)^2]\\)\nthey are not the same\n\nb). \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\)\nIs not time-invariant, because:\n\nwith \\(x[n-k]\\) we have \\(\\mathcal{H}\\left\\{x[n-k]\\right\\} = x[n-k] \\cdot \\cos(\\omega_0 n)\\)\ndelaying the output by \\(k\\) we have \\(y[n-k] = x[n-k] \\cdot \\cos(\\omega_0 (n-k))\\)\nthey are not the same\n\nc). \\(y[n] = \\sin(x[n])\\)\nIs time-invariant, because:\n\nwith \\(x[n-k]\\) we have \\(\\mathcal{H}\\left\\{x[n-k]\\right\\} = \\sin(x[n-k])\\)\ndelaying the output by \\(k\\) we have \\(y[n-k] = \\sin(x[n-k])\\)\nthey are the same\n\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\nIs not time-invariant, because:\n\nwith \\(x[n-k]\\) we have \\(\\mathcal{H}\\left\\{x[n-k]\\right\\} = x[n-k] + n \\cdot x[(n-k)+1]\\)\ndelaying the output by \\(k\\) we have \\(y[n-k] = x[n-k] + (n-k) \\cdot x[(n-k)+1]\\)\nthey are not the same\n\n\n\n\n\n\n\n\nQuick way of checking time invariance\n\n\n\nTo be time invariant, there should be no \\(n\\) outside the signals \\(x[n]\\), \\(x[n-1]\\), \\(y[n]\\), \\(y[n-1]\\) etc.\nIf there is any \\(n\\) outside the signals, then the system depends on absolute time, and is therefore not time invariant.\nExamples:\n\n\\(y[n] = x[n] + n \\cdot x[n+1]\\) is not time invariant, because of the \\(n\\) outside the signals\n\\(y[n] = \\sin(x[n])\\) is time invariant, because there is no \\(n\\) outside the signals, only within parantheses of \\(x[n]\\)\n\n\n\n\n\nCausality\nA system is causal if the output at time \\(n\\) depends only on the input at time \\(n\\) and in the past, i.e. \\(x[n-k]\\), but never on the future, i.e. \\(x[n+1]\\).\nLet’s see the systems one by one:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nIs not causal, because \\(y[2] = 2 x[4]\\), so \\(y[2]\\) depends on \\(x[4]\\) which is in the future.\nb). \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\)\nIs causal, because \\(y[2] = x[2] \\cdot \\cos(\\omega_0 \\cdot 2)\\), \\(y[3] = x[3] \\cdot \\cos(\\omega_0 \\cdot 3)\\), etc. so every \\(y[n]\\) depends only on \\(x[n]\\) and never on the future.\nc). \\(y[n] = \\sin(x[n])\\)\nIs causal, because \\(y[2] = \\sin(x[2])\\) etc. so every \\(y[n]\\) depends only on \\(x[n]\\) and never on the future.\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\nIs not causal, because \\(y[n]\\) depends on \\(x[n+1]\\), so it depends on the future.\n\n\n\nStability\nA system is stable if the output is bounded for any bounded input. This means that if the input is not going to infinity, then the output must not go to infinity either.\n(Bounded = it does not go to infinity)\nThis is a bit more difficult to check. Basically we look for counterexamples, i.e. when the inputs are bounded (like the unit step 1, 1, 1, 1, … ) but the output goes to infinity. If we can’t find any counterexample, we must find an argument showing that the output never goes to infinity.\nLet’s see the systems one by one:\n\na). \\(y[n] = n \\cdot x[n^2]\\)\nIs not stable because of \\(n\\).\nAssume that the input is \\(x[n] = 1, 1, 1, 1, ...\\). Let’s compute some outputs: \\[y[1] = 1 \\cdot x[1^2] = 1\\] \\[y[100] = 100 \\cdot x[100^2] = 100\\] \\[y[1000000] = 1000000\\]\nEven though the input is always \\(1\\), when \\(n \\to \\infty\\), the output goes to infinity because of \\(n\\).\nb). \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\)\nIs stable, because:\n\n\\(\\cos(\\omega_0 n)\\) is always between -1 and 1\n\\(x[n]\\) is bounded$\nso the output \\(y[n] = x[n] \\cdot \\cos(\\omega_0 n)\\) is always bounded if \\(x[n]\\) is bounded.\n\nc). \\(y[n] = \\sin(x[n])\\)\nIs stable, because the values of a \\(\\sin\\) are always between -1 and 1, no matter what the input is.\nd). \\(y[n] = x[n] + n \\cdot x[n+1]\\)\nIs not stable because of \\(n\\), same argument as in a).",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html#exercise-4",
    "href": "Ex02_Systems.html#exercise-4",
    "title": "4  Signals and Systems",
    "section": "4.4 Exercise 4",
    "text": "4.4 Exercise 4\nCompute the convolution of the signals: \\(x_1[n] = \\{ ...,0,1,2,\\underuparrow{3},4,0,...\\}\\) and \\(x_2[n] = \\{...,0,2,\\underuparrow{2},3,3,0,...\\}\\)\n\nSolution\nThere are three ways of computing the convolution of short signals like these ones.\n\n4.4.0.1 Method 1: using linearity and time invariance\nEach individual value from one signal triggers a copy of the other signal, having the same amplitude and shift as the trigger value.\n\n\n\nConvolution using linearity and time invariance\n\n\n\nthe first value from \\(x_1[n]\\), which is \\(1\\), triggers a copy of \\(x_2[n]\\) multipled by \\(1\\)\nthe second value from \\(x_1[n]\\), which is \\(2\\) and shifted by one step, triggers a copy of \\(x_2[n]\\) multipled by \\(2\\) and shifted by one step\nthe third value from \\(x_1[n]\\), which is \\(3\\) and shifted by two steps, triggers a copy of \\(x_2[n]\\) multipled by \\(3\\) and shifted by two steps\nthe fourth value from \\(x_1[n]\\), which is \\(4\\) and shifted by three steps, triggers a copy of \\(x_2[n]\\) multipled by \\(4\\) and shifted by three steps\n\nThe convolution result is the sum of all these copies, which is: \\[y[n] = \\{..., 2, 6, 13, \\underuparrow{23}, 23, 21, 12, ... \\}\\]\nThe origin of time in the resulting sequence is at the value which corresponds to the origin of time in the two input sequences (see the light blue arrows).\n\n\n4.4.0.2 Method 2: using the definition\nThe convolution of two signals is defined as: \\[y[n] = \\sum_{k=-\\infty}^{\\infty} x_1[k] \\cdot x_2[n-k]\\]\n\n\\(x_1[k]\\) is the first signal\n\\(x_2[n-k]\\) is the second signal, reversed and then shifted by \\(n\\) steps to the right\nfor every shift \\(n\\), multiply these two and sum\n\nWe write \\(x_1[k]\\) once, and then we write all the shifted and reversed copies of \\(x_2[n-k]\\), we multiply them point by point with \\(x_1[k]\\) and sum the result. In this way we compute all the values \\(y[n]\\), one by one.\nThe origin of time in \\(y[n]\\) corresponds to the value calculated when the time origins of the two signals are aligned.\n\n\n\nConvolution using the definition\n\n\n\n\n\n4.4.1 Method 3: using the Z transform (polynomials)\nWe can write the convolution as a product of the Z transforms of the two signals: \\[x_1[n] \\leftrightarrow X_1(z) = 1z^2 + 2z^1 + 3 + 4z^{-1}\\] \\[x_2[n] \\leftrightarrow X_2(z) = 2z^1 + 2 + 3z^{-1} + 3z^{-2}\\] \\[\\begin{aligned}\nx_1[n] \\ast x_2[n] \\leftrightarrow X_1 \\cdot X_2(z) &= (1z^2 + 2z^1 + 3 + 4z^{-1}) \\cdot (2z^1 + 2 + 3z^{-1} + 3z^{-2}) \\\\\n&= 2z^3 + 6z^2 + 13z + 23 + 23z^{-1} + 21z^{-2} + 12z^{-3}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex02_Systems.html#exercise-5",
    "href": "Ex02_Systems.html#exercise-5",
    "title": "4  Signals and Systems",
    "section": "4.5 Exercise 5",
    "text": "4.5 Exercise 5\nCompute the 2D convolution of the image \\[I = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n2 & 2 & 2 & 2 & 2 \\\\\n3 & 3 & 3 & 3 & 3 \\\\\n\\end{bmatrix}\\] with the kernel image: \\[H = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & -4 & 1 \\\\\n0 & 1 & 0 \\\\\n\\end{bmatrix}\\]\nNote: the result must be the same shape as the input signal.\n\nSolution\nWe proceed similarly to Method 2 in the previous exercise.\nWe write the input matrix \\(I\\) once, then reverse \\(H\\) (flip horizontally and vertically), then we shift \\(H\\) across all the positions of \\(I\\), multiply point by point and sum.\nHere, because \\(H\\) is symmetric, flipping it horizontally and vertically makes no difference.\nThe resulting matrix is: \\[\\begin{bmatrix}\n-1 & 0 & 0 & 0 & -1 \\\\\n-2 & 0 & 0 & 0 & -2 \\\\\n-7 & -4 & -4 & -4 & -7 \\\\\n\\end{bmatrix}\\]\n\n\n\n2D convolution",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Signals and Systems</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html",
    "href": "Ex03_Z_Transform.html",
    "title": "5  The Z Transform",
    "section": "",
    "text": "5.1 Exercise 1\nCompute the convolution of the signals: \\(x_1[n] = \\{ ...,0,1,2,\\underuparrow{3},4,0,...\\}\\) and \\(x_2[n] = \\{...,0,2,\\underuparrow{2},3,3,0,...\\}\\)",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html#exercise-1",
    "href": "Ex03_Z_Transform.html#exercise-1",
    "title": "5  The Z Transform",
    "section": "",
    "text": "a). using the Z transform\nb). as a product of polynomials\n\n\nSolution\na). Using the Z transform\nWe can write the convolution as a product of the Z transforms of the two signals: \\[x_1[n] \\leftrightarrow X_1(z) = 1z^2 + 2z^1 + 3 + 4z^{-1}\\] \\[x_2[n] \\leftrightarrow X_2(z) = 2z^1 + 2 + 3z^{-1} + 3z^{-2}\\] \\[\\begin{aligned}\nx_1[n] \\ast x_2[n] \\leftrightarrow X_1 \\cdot X_2(z) &= (1z^2 + 2z^1 + 3 + 4z^{-1}) \\cdot (2z^1 + 2 + 3z^{-1} + 3z^{-2}) \\\\\n&= 2z^3 + 6z^2 + 13z + 23 + 23z^{-1} + 21z^{-2} + 12z^{-3} \\\\\n&= \\{2, 6, 13, \\underuparrow{23}, 23, 21, 12\\} \\\\\n\\end{aligned}\n\\]\nb). As a product of polynomials\nThe Z transform is simply associating a polynomial to a signal, according to some rules (variable is \\(z\\), powers are in decreasing order). Convolution is a polynomial multiplication, irrespective of how we define the polynomials (like in Z transform, or in some other way).\nWe could also associate any polynomials and compute the convolution as a product of those polynomials. \\[\\{1, 2, 3, 4\\} \\rightarrow 1 + 2X + 3X^2 + 4X^3\\] \\[\\{2, 2, 3, 3\\} \\rightarrow 2 + 2X + 3X^2 + 3X^3\\] \\[\\begin{aligned}\nx_1[n] \\ast x_2[n] &\\rightarrow (1 + 2X + 3X^2 + 4X^3) \\cdot (2 + 2X + 3X^2 + 3X^3) \\\\\n&= 2 + 6X + 13X^2 + 23X^3 + 23X^4 + 21X^5 + 12X^6 \\\\\n&= \\{2, 6, 13, 23, 23, 21, 12\\} \\\\\n\\end{aligned}\\]\nThe origin of time is at the element containing the product of the two values at the origin of time in the two input signals, i.e. \\(3X^2\\) from \\(x_1[n]\\) and \\(2X\\) from \\(x_2[n]\\) is \\(6X^3\\), so it’s the element with \\(X^3\\) in the output signal.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html#exercise-2",
    "href": "Ex03_Z_Transform.html#exercise-2",
    "title": "5  The Z Transform",
    "section": "5.2 Exercise 2",
    "text": "5.2 Exercise 2\nFind the Z transform of the following signals:\n\na). \\[x[n] = \\left( \\frac{1}{3} \\right)^n u[n]\\]\nb). \\[x[n] = \\begin{cases}\n  \\left( \\frac{1}{3} \\right)^n, & n \\geq 0 \\\\\n  \\left( \\frac{1}{2} \\right)^{-n}, & n &lt; 0\n  \\end{cases}\\]\nc). \\[x[n] = \\left( \\frac{1}{2} \\right)^n \\sin (\\frac{\\pi}{3}n) u[n]\\]\n\n\nSolution\na). We use the definition of the Z transform: \\[X(z) = \\sum_{n=-\\infty}^{\\infty} x[n] z^{-n}\\]\nIn our case, \\(x[n] = \\left( \\frac{1}{3} \\right)^n u[n]\\), so it has only zeros on the left side, because of \\(u[n]\\). Therefore we only need to sum from \\(n=0\\) to \\(\\infty\\): \\[\\begin{aligned}\nX(z) &= \\sum_{n=0}^{\\infty} x[n] z^{-n} \\\\\n&= \\sum_{n=0}^{\\infty} \\left( \\frac{1}{3} \\right)^n z^{-n} \\\\\n&= \\sum_{n=0}^{\\infty} \\left( \\frac{1}{3z} \\right)^n \\\\\n\\end{aligned}\\]\nDenoting \\(q = \\frac{1}{3z}\\), we have here the sum of a geometric series: \\[X(z) = \\sum_{n=0}^{\\infty} q^n  = 1 + q + q^2 +...\\]\nIt is known that the sum of a geometric series with element \\(q\\) is: \\[\\sum_{n=0}^{\\infty} q^n = \\frac{1}{1-q}\\] if \\(|q| &lt; 1\\). The sum in infinite otherwise (\\(|q| \\geq 1\\)).\nTherefore in our case, \\(q = \\frac{1}{3z}\\), so if we have \\(|\\frac{1}{3z}| &lt; 1\\), the sum is: \\[X(z) = \\frac{1}{1-q} = \\frac{1}{1-\\frac{1}{3z}} = \\frac{3z}{3z-1} = \\frac{z}{z-\\frac{1}{3}}\\]\nThe condition \\(|\\frac{1}{3z}| &lt; 1\\) defines the Region of Convergence (ROC). We can more conveniently write it as: \\[|\\frac{1}{3z}| &lt; 1 \\rightarrow |z| &gt; \\frac{1}{3}\\]\nTherefore the answer is: \\[X(z) = \\frac{z}{z-\\frac{1}{3}}, \\quad \\text{ROC:}|z| &gt; \\frac{1}{3}\\]\nb). We use the definition of the Z transform, but now we also have values on the left side of \\(x[n]\\). We split the sum in two parts: \\[\\begin{aligned}\nX(z) &= \\sum_{n=-\\infty}^{\\infty} x[n] z^{-n} \\\\\n&= \\sum_{n=-\\infty}^{-1} x[n] z^{-n} + \\sum_{n=0}^{\\infty} x[n] z^{-n} \\\\\n&= \\sum_{n=-\\infty}^{-1} \\left( \\frac{1}{2} \\right)^{-n} z^{-n} + \\sum_{n=0}^{\\infty} \\left( \\frac{1}{3} \\right)^n z^{-n} \\\\\n&= \\sum_{n=-\\infty}^{-1} \\left( \\frac{z}{2} \\right)^{-n} + \\sum_{n=0}^{\\infty} \\left( \\frac{1}{3z} \\right)^n \\\\\n\\end{aligned}\\]\nThe second sum is similar to the one in the previous exercise, so it results in: \\[\\sum_{n=0}^{\\infty} \\left( \\frac{1}{3z} \\right)^n = \\frac{z}{z-\\frac{1}{3}}\n\\quad \\text{if} \\quad |z| &gt; \\frac{1}{3}\\]\nThe first sum is also a geometric series. First, we can do a variable change \\(n = -m\\), so that we have a sum from \\(m=1\\) to \\(\\infty\\) instead: \\[\\sum_{n=-\\infty}^{-1} \\left( \\frac{z}{2} \\right)^{-n} = \\sum_{m=1}^{\\infty} \\left( \\frac{z}{2} \\right)^{m}\\]\nThis is a geometric series with element \\(q = \\frac{z}{2}\\), but the first element \\(q^0 = 1\\) is missing, so we have to remove it from the sum: \\[\\begin{aligned}\n\\sum_{m=1}^{\\infty} \\left( \\frac{z}{2} \\right)^{m} &= \\sum_{m=0}^{\\infty} \\left( \\frac{z}{2} \\right)^{m} - 1 \\\\\n&= \\frac{1}{1-\\frac{z}{2}} - 1 \\\\\n&= \\frac{1}{1-\\frac{z}{2}} - \\frac{1-\\frac{z}{2}}{1-\\frac{z}{2}} \\\\\n&= \\frac{\\frac{z}{2}}{1-\\frac{z}{2}} \\\\\n&= - \\frac{z}{z-2}\n\\end{aligned}\\]\nThe condition is \\(|\\frac{z}{2}| &lt; 1\\), which is equivalent to \\(|z| &lt; 2\\).\nAdding the two sums, we have: \\[\\begin{aligned}\nX(z) &= - \\frac{z}{z-2} + \\frac{z}{z-\\frac{1}{3}} \\\\\n&= \\frac{-z(z-\\frac{1}{3}) + z(z-2)}{(z-2)(z-\\frac{1}{3})} \\\\\n&= \\frac{-\\frac{5}{3}z}{(z-2)(z-\\frac{1}{3})} \\\\\n\\end{aligned}\\]\nThe ROC is the intersection of the ROCs of the two sums, which is: \\[\\text{ROC:} \\quad |z| &gt; \\frac{1}{3} \\quad \\text{and} \\quad |z| &lt; 2\\] \\[\\text{ROC:} \\quad \\frac{1}{3} &lt; |z| &lt; 2\\]\nc). We can write the \\(\\sin\\) using the Euler formula: \\[\\sin(x) = \\frac{e^{jx} - e^{-jx}}{2j}\\] \\[\\sin (\\frac{\\pi}{3}n) = \\frac{e^{j\\frac{\\pi}{3}n} - e^{-j\\frac{\\pi}{3}n}}{2j}\\]\nThe \\(u[n]\\) cancels the left part of the signal, so the Z transform sum starts from 0: \\[\\begin{aligned}\nX(z) &= \\sum_{n=0}^{\\infty} \\left( \\frac{1}{2} \\right)^n \\frac{e^{j\\frac{\\pi}{3}n} - e^{-j\\frac{\\pi}{3}n}}{2j} z^{-n} \\\\\n&= \\frac{1}{2j} \\sum_{n=0}^{\\infty} \\left( \\frac{e^{j\\frac{\\pi}{3}}}{2z} \\right)^n - \\frac{1}{2j} \\sum_{n=0}^{\\infty} \\left( \\frac{e^{-j\\frac{\\pi}{3}}}{2z} \\right)^n \\\\\n\\end{aligned}\\] (since \\(e^{j\\frac{\\pi}{3}n} = (e^{j\\frac{\\pi}{3}})^n\\)).\nFrom now on, we use the same geometric series formula: \\[\\frac{1}{2j} \\sum_{n=0}^{\\infty} \\left( \\frac{e^{j\\frac{\\pi}{3}}}{2z} \\right)^n = \\frac{1}{2j} \\frac{1}{1 - q} = \\frac{1}{2j} \\frac{1}{1 - \\frac{e^{j\\frac{\\pi}{3}}}{2z}}\\] \\[\\frac{1}{2j} \\sum_{n=0}^{\\infty} \\left( \\frac{e^{-j\\frac{\\pi}{3}}}{2z} \\right)^n = \\frac{1}{2j} \\frac{1}{1 - q} = \\frac{1}{2j} \\frac{1}{1 - \\frac{e^{-j\\frac{\\pi}{3}}}{2z}}\\] with the conditions \\(|\\frac{e^{j\\frac{\\pi}{3}}}{2z}| &lt; 1\\) and \\(|\\frac{e^{-j\\frac{\\pi}{3}}}{2z}| &lt; 1\\), which are equivalent to \\(|z| &gt; \\frac{1}{2}\\).\nAdding the two sums, we have: \\[\\begin{aligned}\nX(z) &= \\frac{1}{2j} \\frac{1}{1 - \\frac{e^{j\\frac{\\pi}{3}}}{2z}} - \\frac{1}{2j} \\frac{1}{1 - \\frac{e^{-j\\frac{\\pi}{3}}}{2z}} \\\\\n&= \\frac{1}{2j} \\frac{\\frac{e^{j\\frac{\\pi}{3}} - e^{-j\\frac{\\pi}{3}}}{2z}}{(1 - \\frac{e^{j\\frac{\\pi}{3}}}{2z})(1 - \\frac{e^{-j\\frac{\\pi}{3}}}{2z})} \\\\\n&= \\frac{\\frac{1}{2z}\\sin(\\frac{\\pi}{3})}{(1 - \\frac{e^{j\\frac{\\pi}{3}}}{2z})(1 - \\frac{e^{-j\\frac{\\pi}{3}}}{2z})} \\\\\n&= \\frac{\\sin(\\frac{\\pi}{3})}{(2z - e^{j\\frac{\\pi}{3}})(2z - e^{-j\\frac{\\pi}{3}})} \\\\\n&= \\frac{\\sin(\\frac{\\pi}{3})}{4z^2 - 2ze^{j\\frac{\\pi}{3}} - 2ze^{-j\\frac{\\pi}{3}} + 1} \\\\\n&= \\frac{\\sin(\\frac{\\pi}{3})}{4z^2 - 2z(e^{j\\frac{\\pi}{3}} + e^{-j\\frac{\\pi}{3}}) + 1} \\\\\n&= \\frac{\\sin(\\frac{\\pi}{3})}{4z^2 - 2z(2\\cos(\\frac{\\pi}{3})) + 1} \\\\\n&= \\frac{\\sin(\\frac{\\pi}{3})}{4z^2 - 4\\cos(\\frac{\\pi}{3})z + 1} \\\\\n&= \\frac{\\frac{\\sqrt{3}}{2}}{4z^2 - 2z + 1} \\\\\n&= \\frac{\\sqrt{3}}{8} \\frac{1}{z^2 - \\frac{1}{2}z + \\frac{1}{4}} \\\\\n\\end{aligned}\\] \\[\\text{ROC:} \\quad |z| &gt; \\frac{1}{2}\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html#exercise-3",
    "href": "Ex03_Z_Transform.html#exercise-3",
    "title": "5  The Z Transform",
    "section": "5.3 Exercise 3",
    "text": "5.3 Exercise 3\nFind all the signals \\(x[n]\\) which have the Z transform \\[X(z) = \\frac{7}{(1 - 2z^{-1})(1+0.5z^{-1})}\\]\n\nSolution\nBecause we don’t specify the ROC, we will have multiple signals which have the same expression of the Z transform, but with different ROCs.\nWe find the inverse Z transform by using the partial fraction expansion.\nFirst we get rid of the negative powers of \\(z\\), to make calculations easier: \\[X(z) = \\frac{7}{(1 - 2z^{-1})(1+0.5z^{-1})} = \\frac{7z^2}{(z - 2)(z+0.5)}\\]\nThen we write \\(X(z)/z\\): \\[\\frac{X(z)}{z} = \\frac{7z}{(z - 2)(z+0.5)}\\]\nThen we write the partial fractions: \\[\\frac{X(z)}{z} = \\frac{7z}{(z - 2)(z+0.5)} = \\frac{A}{z-2} + \\frac{B}{z+0.5}\\]\nWe can find \\(A\\) and \\(B\\) now, or leave them for later. \\[A = \\frac{7z}{z+0.5} \\Big|_{z=2} = \\frac{7 \\cdot 2}{2 + 0.5} = \\frac{14}{2.5}\\] \\[B = \\frac{7z}{z-2} \\Big|_{z=-0.5} = \\frac{7 \\cdot (-0.5)}{-0.5 - 2} = \\frac{-3.5}{-2.5}\\]\nNext we multiply back with \\(z\\): \\[X(z) = A \\frac{z}{z-2} + B \\frac{z}{z+0.5}\\]\nNow we use the inverse Z transform table to invert the partial fractions: \\[\\begin{aligned}\na^n u[n] &\\leftrightarrow \\frac{z}{z-a}, \\text{ for } |z| &gt; |a| \\\\\n-a^n u[-n-1] &\\leftrightarrow \\frac{z}{z-a}, \\text{ for } |z| &lt; |a| \\\\\n\\end{aligned}\\]\nBecause the ROC is not specified, we will have multiple possible signals for each of the partial fractions.\n\nCase 1: if \\(|z| &lt; 0.5 &lt; 2\\), we use the variant with \\(u[-n-1]\\) for both fractions: \\[\\begin{aligned}\n\\frac{z}{z-2} &\\leftrightarrow -2^n u[-n-1] \\\\\n\\frac{z}{z+0.5} &\\leftrightarrow -(-0.5)^n u[-n-1]\n\\end{aligned}\\] Therefore the signal is: \\[x_1[n] = - \\frac{14}{2.5} 2^n u[-n-1] - \\frac{3.5}{2.5} (-0.5)^n u[-n-1]\\]\nCase 2: if \\(0.5 &lt; |z| &lt; 2\\), we use the variant with \\(u[n]\\) for the fraction with \\(0.5\\) and \\(u[-n-1]\\) for the fraction with \\(2\\): \\[\\begin{aligned}\n\\frac{z}{z-2} &\\leftrightarrow -2^n u[-n-1] \\\\\n\\frac{z}{z+0.5} &\\leftrightarrow (-0.5)^n u[n]\n\\end{aligned}\\] Therefore the signal is: \\[x_2[n] = - \\frac{14}{2.5} 2^n u[-n-1] + \\frac{3.5}{2.5} (-0.5)^n u[n]\\]\nCase 3: if \\(|z| &gt; 2 &gt; 0.5\\), we use the variant with \\(u[n]\\) for both fractions: \\[\\begin{aligned}\n\\frac{z}{z-2} &\\leftrightarrow 2^n u[n] \\\\\n\\frac{z}{z+0.5} &\\leftrightarrow (-0.5)^n u[n]\n\\end{aligned}\\] Therefore the signal is: \\[x_3[n] = \\frac{14}{2.5} 2^n u[n] + \\frac{3.5}{2.5} (-0.5)^n u[n]\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html#exercise-4",
    "href": "Ex03_Z_Transform.html#exercise-4",
    "title": "5  The Z Transform",
    "section": "5.4 Exercise 4",
    "text": "5.4 Exercise 4\nConsider the system with the following system equation: \\[y[n] = -0.1 y[n-1] + 0.2 y[n-2] + x[n] + x[n-1]\\]\n\na). Find the system function H(z), draw the pole-zero diagram, and specify if the system is stable\nb). Compute the impulse response \\(h[n]\\) of the system\nc). Compute the response of the system to the unit step \\(x[n] = u[n]\\)\nd). Compute the response of the system to the input signal \\(x[n] = \\left(\\frac{1}{3}\\right)^n u[n]\\)\n\n\nSolution\na). Find the system function H(z), draw the pole-zero diagram, and specify if the system is stable\nWe can find H(z) by taking the Z transform of the system equation: \\[\\begin{aligned}\nY(z) &= -0.1 z^{-1} Y(z) + 0.2 z^{-2} Y(z) + X(z) + z^{-1} X(z) \\\\\nY(z) (1 + 0.1 z^{-1} - 0.2 z^{-2}) &= X(z) (1 + z^{-1}) \\\\\nH(z) = \\frac{Y(z)}{X(z)} &= \\frac{1 + z^{-1}}{1 + 0.1 z^{-1} - 0.2 z^{-2}} \\\\\n&= \\frac{z(z + 1)}{z^2 + 0.1 z - 0.2} \\\\\n\\end{aligned}\\]\n\n\n\n\n\n\nQuick way of writing \\(H(z)\\)\n\n\n\nThe coefficients of \\(x[n-k]\\) are the coefficients \\(b_k\\) at the numerator of \\(H(z)\\). Here, they are \\(b_0 = 1\\) and \\(b_1 = 1\\).\nThe coefficients of \\(y[n-k]\\) are the coefficients \\(a_k\\) at the denominator of \\(H(z)\\), with the opposite sign. Here, they are \\(a_1 = 0.1\\) and \\(a_2 = -0.2\\) (note the different sign from how they appear in the equation).\nTherefore \\(H(z)\\) is: \\[H(z) = \\frac{1 + z^{-1}}{1 + 0.1 z^{-1} - 0.2 z^{-2}}\\]\n\n\nFind the zeros as the roots of the numerator polynomial: \\[z(z + 1) = 0 \\rightarrow z_1 = 0, \\quad z_2 = -1\\] Find the poles as the roots of the denominator polynomial: \\[z^2 + 0.1 z^ - 0.2 = 0 \\rightarrow p_1 = -0.5, \\quad p_2 = 0.4\\]\nThe pole-zero diagram is:\n\n\n\nPole-zero diagram\n\n\nThe system is stable, because:\n\nThe system is causal, because the system equation is causal (no \\(y[n+k]\\) or \\(x[n+k]\\) terms).\nA causal system is stable if all the poles are inside the unit circle. In our case, the poles are \\(p_1 = -0.5\\) and \\(p_2 = 0.4\\), so they are indeed inside the unit circle.\n\nb). Compute the impulse response \\(h[n]\\) of the system\nThe impulse response \\(h[n]\\) is the inverse Z transform of the system function \\(H(z)\\). We use the same method for inverting the Z transform with partial fractions as in a previous exercise.\nWe start from the expression of \\(H(z)\\), with positive exponents: \\[H(z) = \\frac{z(z + 1)}{z^2 + 0.1 z - 0.2}= \\frac{z(z+1)}{(z+0.5)(z-0.4)}\\] Then we write \\(H(z)/z\\): Write \\(H(z)/z\\): \\[\\frac{H(z)}{z} = \\frac{z + 1}{(z+0.5)(z-0.4)}\\] Then decompose in partial fractions: \\[\\frac{H(z)}{z} = \\frac{z + 1}{(z+0.5)(z-0.4)} = \\frac{A}{z+0.5} + \\frac{B}{z-0.4}\\] Multiply back with \\(z\\): \\[H(z) = A \\frac{z}{z+0.5} + B \\frac{z}{z-0.4}\\]\nWe know the system is causal, so we know \\(h[n]\\) must be causal, so we invert the fractions using the variant with \\(u[n]\\) for both: \\[\\begin{aligned}\n\\frac{z}{z+0.5} &\\leftrightarrow (-0.5)^n u[n] \\\\\n\\frac{z}{z-0.4} &\\leftrightarrow (0.4)^n u[n]\n\\end{aligned}\\]\nThe result is: \\[h[n] = A (-0.5)^n u[n] + B (0.4)^n u[n]\\] where \\(A\\) and \\(B\\) are: \\[A = \\frac{z+1}{z-0.4} \\Big|_{z=-0.5} = \\frac{-0.5+1}{-0.5-0.4} = \\frac{0.5}{-0.9}\\] \\[B = \\frac{z+1}{z+0.5} \\Big|_{z=0.4} = \\frac{0.4+1}{0.4+0.5} = \\frac{1.4}{0.9}\\]\nc). Compute the response of the system to the unit step \\(x[n] = u[n]\\)\nIf the system function is \\(H(z)\\), and the input has the Z transform \\(X(z)\\), then the output Z transform is: \\[Y(z) = H(z) X(z)\\]\nIn our case, the input is \\(x[n] = u[n]\\), so its Z transform is: \\[X(z) = \\frac{1}{1 - z^{-1}} = \\frac{z}{z-1}, \\quad \\text{ROC:} \\quad |z| &gt; 1\\]\nTherefore: \\[\\begin{aligned}\nY(z) &= H(z) X(z) \\\\\n&= \\frac{z(z+1)}{(z+0.5)(z-0.4)} \\cdot \\frac{z}{z-1} \\\\\n&= \\frac{z^2(z+1)}{(z+0.5)(z-0.4)(z-1)} \\\\\n\\end{aligned}\\]\nNow we use the same inverse Z procedure as in the previous exercises.\nWrite \\(Y(z)/z\\), then decompose in partial fractions: \\[\\frac{Y(z)}{z} = \\frac{z(z+1)}{(z+0.5)(z-0.4)(z-1)} = \\frac{A}{z+0.5} + \\frac{B}{z-0.4} + \\frac{C}{z-1}\\] Multiply back with \\(z\\): \\[Y(z) = A \\frac{z}{z+0.5} + B \\frac{z}{z-0.4} + C \\frac{z}{z-1}\\]\nThe system is causal, the input is causal (\\(u[n]\\)), so the output must be causal, so we invert using the variant with \\(u[n]\\) for all fractions: \\[y[n] = A (-0.5)^n u[n] + B (0.4)^n u[n] + C u[n]\\] where \\(A\\), \\(B\\) and \\(C\\) are: \\[A = \\frac{z(z+1)}{(z-0.4)(z-1)} \\Big|_{z=-0.5} = \\frac{-0.5(-0.5+1)}{(-0.5-0.4)(-0.5-1)} = ...\\] \\[B = \\frac{z(z+1)}{(z+0.5)(z-1)} \\Big|_{z=0.4} = \\frac{0.4(0.4+1)}{(0.4+0.5)(0.4-1)} = ...\\] \\[C = \\frac{z(z+1)}{(z+0.5)(z-0.4)} \\Big|_{z=1} = \\frac{1(1+1)}{(1+0.5)(1-0.4)} = ...\\] and the ROC is the intersection of the ROCs of the three fractions, which is: \\[\\text{ROC:} \\quad |z| &gt; |-0.5| \\quad \\text{and} \\quad |z| &gt; |0.4| \\quad \\text{and} \\quad |z| &gt; |1|\\] which means: \\[\\text{ROC:} \\quad |z| &gt; 1\\]\nd). Compute the response of the system to the input signal \\(x[n] = \\left(\\frac{1}{3}\\right)^n u[n]\\)\nThe Z tranform of the input is: \\[X(z) = \\frac{1}{1 - \\frac{1}{3} z^{-1}} = \\frac{z}{z-\\frac{1}{3}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\]\nWe solve in the same manner as in c).",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex03_Z_Transform.html#exercise-5",
    "href": "Ex03_Z_Transform.html#exercise-5",
    "title": "5  The Z Transform",
    "section": "5.5 Exercise 5",
    "text": "5.5 Exercise 5\nA causal LTI system has the property that if the input signal is \\[x[n] = \\left( \\frac{1}{3} \\right)^n u[n] - \\frac{1}{4} \\left( \\frac{1}{3} \\right)^n u[n-1],\\] then the output signal is \\[y[n] = \\left( \\frac{1}{4} \\right)^n u[n]\\]\n\na). Find the system function H(z), draw the pole-zero diagram\nb). Compute the impulse response \\(h[n]\\) of the system\nc). Find the difference equation of the system\nd). Characterize the system with respect to:\n\nlength of impulse response (FIR or IIR)\nimplementation (recursive or non-recursive)\nstability\n\n\n\nSolution\na). Find the system function H(z), draw the pole-zero diagram\nWe know that \\(H(z) = \\frac{Y(z)}{X(z)}\\). We find \\(X(z)\\) and \\(Y(z)\\), then we divide them to obtain \\(H(z)\\).\nFor the Z transform of the input, we take each term: \\[\\left(\\frac{1}{3}\\right)^n u[n] \\leftrightarrow \\frac{z}{z-\\frac{1}{3}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\] For the second term, observe that we can write it as: \\[\\frac{1}{4} \\left( \\frac{1}{3} \\right)^n u[n-1] = \\frac{1}{4} \\cdot \\frac{1}{3} \\cdot \\left( \\frac{1}{3} \\right)^{n-1} u[n-1]\\] and \\(\\left( \\frac{1}{3} \\right)^{n-1} u[n-1]\\) is the shifted version of \\(\\left( \\frac{1}{3} \\right)^n u[n]\\), so its Z transform is multiplied by \\(z^{-1}\\): \\[\\left( \\frac{1}{3} \\right)^{n-1} u[n-1] \\leftrightarrow z^{-1} \\frac{z}{z-\\frac{1}{3}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\] Therefore the second term has the Z transform: \\[\\frac{1}{4} \\left( \\frac{1}{3} \\right)^n u[n-1] \\leftrightarrow \\frac{1}{12} \\cdot \\frac{1}{z-\\frac{1}{3}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\] and the Z transform of the whole input \\(x[n]\\) is: \\[X(z) = \\frac{z}{z-\\frac{1}{3}} - \\frac{1}{12} \\cdot \\frac{1}{z-\\frac{1}{3}} = \\frac{z-\\frac{1}{12}}{z-\\frac{1}{3}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\]\nThe Z transform of the output is: \\[Y(z) = \\frac{z}{z-\\frac{1}{4}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{4}\\]\nTherefore the system function is the ratio of the two, and the ROC is the intersection of the ROCs of the two: \\[\\begin{aligned}\nH(z) = \\frac{Y(z)}{X(z)} &= \\frac{z}{z-\\frac{1}{4}} \\cdot \\frac{z-\\frac{1}{3}}{z-\\frac{1}{12}}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3} \\quad \\text{and} \\quad |z| &gt; \\frac{1}{4}\\\\\n&= \\frac{z (z-\\frac{1}{3})}{(z-\\frac{1}{4})(z-\\frac{1}{12})}, \\quad \\text{ROC:} \\quad |z| &gt; \\frac{1}{3}\\\\\n\\end{aligned}\\]\nThe zeros are the roots of the numerator polynomial, i.e. \\(z_1 = 0\\) and \\(z_2 = \\frac{1}{3}\\).\nThe poles are the roots of the denominator polynomial, i.e. \\(p_1 = \\frac{1}{4}\\) and \\(p_2 = \\frac{1}{12}\\).\nThe pole-zero diagram is:\n\n\n\nPole-zero diagram\n\n\nb). Compute the impulse response \\(h[n]\\) of the system\nWe compute \\(h[n]\\) as the inverse Z transform of \\(H(z)\\), using the same method as in the previous exercises.\nThe system is causal (it is mentioend in the text, and also because the ROC is the outside of a circle), so \\(h[n]\\) must be causal, which means we invert the fractions using the variant with \\(u[n]\\) for all fractions.\nc). Find the difference equation of the system\nWe disband the parantheses in \\(H(z)\\) and simplify to obtain negative exponents: \\[\\begin{aligned}\nH(z) &= \\frac{z (z-\\frac{1}{3})}{(z-\\frac{1}{4})(z-\\frac{1}{12})} \\\\\n&= \\frac{z^2 - \\frac{1}{3}z}{z^2 - \\frac{4}{12}z + \\frac{1}{48}}\\\\\n&= \\frac{1 - \\frac{1}{3}z^{-1}}{1 - \\frac{1}{3}z^{-1} + \\frac{1}{48}z^{-2}}\n\\end{aligned}\\]\nWe can write the system equation directly, knowing that the numerator coefficients are the coefficients of \\(x[n-k]\\) and the denominator coefficients, with the opposite sign, are the coefficients of \\(y[n-k]\\): \\[y[n] = \\frac{1}{3} y[n-1] - \\frac{1}{48} y[n-2] + x[n] - \\frac{1}{3} x[n-1]\\]\nWe could also take the long route to the same result: \\[H(z) = \\frac{1 - \\frac{1}{3}z^{-1}}{1 - \\frac{1}{3}z^{-1} + \\frac{1}{48}z^{-2}} = \\frac{Y(z)}{X(z)}\\] \\[Y(z) (1 - \\frac{1}{3}z^{-1} + \\frac{1}{48}z^{-2}) = X(z) (1 - \\frac{1}{3}z^{-1})\\] \\[y[n] - \\frac{1}{3} y[n-1] + \\frac{1}{48} y[n-2] = x[n] - \\frac{1}{3} x[n-1]\\] \\[y[n] = \\frac{1}{3} y[n-1] - \\frac{1}{48} y[n-2] + x[n] - \\frac{1}{3} x[n-1]\\]\nd). Characterize the system with respect to those three properties\n\nSince the system function \\(H(z)\\) has a denominator (has poles), it is an IIR system, so the length of the impulse response is infinite.\nWe can also see from the expression of \\(h[n]\\) which we obtained in b).\nThe system is recursive, because it has a feedback loop (\\(y[n]\\) depends on \\(y[n-1]\\) and \\(y[n-2]\\) in the equation).\nAny IIR system is recursive.\nThe system is stable, because:\n\nit is causal\nthe poles are inside the unit circle (\\(p_1 = \\frac{1}{4}\\), \\(p_2 = \\frac{1}{12}\\)).",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Z Transform</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html",
    "href": "Ex04_Fourier.html",
    "title": "6  The Fourier Transforms",
    "section": "",
    "text": "6.1 Exercise 1\nFind the DTFT of the signal \\(\\{..., 0, \\underuparrow{1}, 1, 1, 0, 0, ...\\}\\)",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-1",
    "href": "Ex04_Fourier.html#exercise-1",
    "title": "6  The Fourier Transforms",
    "section": "",
    "text": "a). Write the expression of \\(|X(\\omega)|\\) and \\(\\angle{X(\\omega)}\\)\nb). What is the signal’s spectrum (modulus and phase) at frequency \\(f=\\frac{1}{2}\\)?\n\n\nSolution\na). Write the expression of \\(|X(\\omega)|\\) and \\(\\angle{X(\\omega)}\\)\nWe apply the definition of the DTFT: \\[X(\\omega) = \\sum_{n=-\\infty}^{\\infty} x[n] e^{-j\\omega n}\\]\nOur signal \\(x[n]\\) has non-zero values only for \\(n=0,1,2\\), so we can restrict the sum to these three terms: \\[\\begin{aligned}\nX(\\omega) &= \\sum_{n=0}^{2} x[n] e^{-j\\omega n} \\\\\n&= x[0] e^{-j\\omega 0} + x[1] e^{-j\\omega 1} + x[2] e^{-j\\omega 2} \\\\\n&= 1 + e^{-j\\omega} + e^{-j2\\omega}\n\\end{aligned}\\]\nUsing the Euler formula: \\[\\begin{aligned}\ne^{j\\omega} &= \\cos(\\omega) + j \\sin(\\omega) \\\\\ne^{-j\\omega} &= \\cos(-\\omega) + j \\sin(-\\omega) = \\cos(\\omega) - j \\sin(\\omega)\n\\end{aligned}\\] we can write: \\[\\begin{aligned}\nX(\\omega) &= 1 + e^{-j\\omega} + e^{-j2\\omega} \\\\\n&= 1 + (\\cos(-\\omega) + j \\sin(-\\omega)) + (\\cos(-2\\omega) + j \\sin(-2\\omega)) \\\\\n&= 1 + \\cos(\\omega) - j \\sin(\\omega) + \\cos(2\\omega) - j \\sin(2\\omega) \\\\\n&= 1 + \\cos(\\omega) + \\cos(2\\omega) - j (\\sin(\\omega) + \\sin(2\\omega))\n\\end{aligned}\\]\nThe real part is \\(1 + \\cos(\\omega) + \\cos(2\\omega)\\), the imaginary part is \\((- \\sin(\\omega) - \\sin(2\\omega))\\), and therefore the modulus and the phase are: \\[\\begin{aligned}\n|X(\\omega)| &= \\sqrt{(1 + \\cos(\\omega) + \\cos(2\\omega))^2 + (\\sin(\\omega) + \\sin(2\\omega))^2} \\\\\n\\angle{X(\\omega)} &= \\arctan\\left(\\frac{- \\sin(\\omega) - \\sin(2\\omega)}{1 + \\cos(\\omega) + \\cos(2\\omega)}\\right)\n\\end{aligned}\\]\nJust for fun, we can plot these functions here:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nplt.rcParams['text.usetex'] = True\n\nw = np.linspace(-np.pi, np.pi, 1000)\nX = 1 + np.cos(w) + np.cos(2*w) - 1j*(np.sin(w) + np.sin(2*w))\n\nplt.figure(figsize=(6, 2))\nplt.plot(w, np.abs(X))\nplt.xlabel(r'$\\omega$')\nplt.ylabel(r'$|X(\\omega)|$')\nplt.title('Modulus of the DTFT transform of the signal')\nplt.tight_layout()\n\nplt.figure(figsize=(6, 2))\nplt.plot(w, np.angle(X))\nplt.xlabel(r'$\\omega$')\nplt.ylabel(r'$\\angle{X(\\omega)}$')\nplt.title('Phase of the DTFT transform of the signal')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb). What is the signal’s spectrum (modulus and phase) at frequency \\(f=\\frac{1}{2}\\)?\nWe have to evaluate the expressions for \\(|X(\\omega)|\\) and \\(\\angle{X(\\omega)}\\) at \\(\\omega = 2\\pi f = 2 \\pi \\frac{1}{2} = \\pi\\):\n\\[\\begin{aligned}\n|X(\\pi)| &= |X(\\omega) \\big|_{\\omega=\\pi}  = \\sqrt{(1 + \\cos(\\pi) + \\cos(2\\pi))^2 + (\\sin(\\pi) + \\sin(2\\pi))^2} = 1 \\\\\n\\angle{X(\\pi)} &= \\angle{X(\\omega)} \\big|_{\\omega=\\pi} = \\arctan\\left(\\frac{- \\sin(\\pi) - \\sin(2\\pi)}{1 + \\cos(\\pi) + \\cos(2\\pi)}\\right) = \\arctan(0) = 0\n\\end{aligned}\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-2",
    "href": "Ex04_Fourier.html#exercise-2",
    "title": "6  The Fourier Transforms",
    "section": "6.2 Exercise 2",
    "text": "6.2 Exercise 2\nCompute the circular convolution of the two signals: \\[x_1[n] = [1, 3, 1, 3]\\] \\[x_2[n] = [2, 2, 5, 5]\\]\n\nSolution\nWe can use the definition of the circular convolution: \\[y[n] = x_1[n] \\circledast x_2[n] = \\sum_{m=0}^{N-1} x_1[m] x_2[(n-m) \\mod N]\\]\nwhere \\(N\\) is the length of the signals. In this case, \\(N=4\\).\nWe follow the same procedure as when calculating the linear convolution, but the indices (positions) are wrapped by \\(N\\). When we exceed the length of the signal, we shift the position back to the beginning.\n\n\n\nCircular convolution vs linear convolution\n\n\nThe result is a sequence with the same length as the input sequences, \\(N=4\\): \\[y[n] = x_1[n] \\circledast x_2[n] = [28, 28, 28, 28]\\]\n\n\n\n\n\n\nCoincidence\n\n\n\nIt is only a coincidence that all the values of \\(y[n]\\) are equal here, \\([28, 28, 28, 28]\\). In general, the result could be anything.\n\n\n\n\n\n\n\n\nGeneralizations\n\n\n\n\nIf the two signals have different lengths, we append zeros to the shorter one until both have the same length.\nWe can pick any value if \\(N\\) longer than the signals. In this case, we extend both signals with zeros until the prescribed length. This is called the circular convolution in N points.\nHomework: compute the circular convolution of these signals in \\(N=6\\) points.\nIf \\(N\\) is large enough, the circular convolution becomes the linear convolution. \\(N\\) must be larger than \\(Length_1 + Length_2 - 1\\).\nHomework: compute the circular convolution of these signals in \\(N=7\\) points.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-3",
    "href": "Ex04_Fourier.html#exercise-3",
    "title": "6  The Fourier Transforms",
    "section": "6.3 Exercise 3",
    "text": "6.3 Exercise 3\nCompute the circular convolution in \\(N = 7\\) points of the same two signals\n\nSolution\nWe must append zeros to both signals to make their length 7, then do circular convolution as before.\nSurprise: we obtain the same result as linear convolution.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-4",
    "href": "Ex04_Fourier.html#exercise-4",
    "title": "6  The Fourier Transforms",
    "section": "6.4 Exercise 4",
    "text": "6.4 Exercise 4\nConsider a periodic signal \\(x[n]\\) with period \\(N=6\\) and the DFT coefficients:\n\\(X_k\\) = [21.0000 + 0.0000i , -3.0000 + 5.1962i , -3.0000 + 1.7321i , -3.0000 + 0.0000i , -3.0000 - 1.7321i , -3.0000 - 5.1962i]\nWrite \\(x[n]\\) as a sum of sinusoids.\n\nSolution\nWe know the connection between the DFT coefficients and the cosine components of the signal, from the theory:\n\nif \\(N\\) is odd: \\[x[n] = \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{(N-1)/2} 2 |X_k| \\cos\\left( 2\\pi \\frac{k}{N} n + \\angle{X_k}\\right)\\]\nif \\(N\\) is even: \\[x[n] = \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{N/2-1} 2 |X_k| \\cos\\left( 2\\pi \\frac{k}{N} n + \\angle{X_k}\\right) + \\frac{1}{N} |X_{N/2}| \\cos\\left( \\pi n + \\angle{X_{N/2}}\\right)\\]\n\nIn our case we have \\(N=6\\), so we use the second formula.\nWe compute the modulus and phase of the DFT coefficients:\n\n\\(X_0 = 21\\): is a real number, positive\n\n\\(|X_0| = 21\\)\n\\(\\angle{X_0} = 0\\)\n\n\\(X_1 = -3.0000 + j5.1962\\):\n\n\\(|X_1| = \\sqrt{(-3)^2 + 5.1962^2} = ...\\)\n\\(\\angle{X_1} = \\arctan\\left(\\frac{5.1962}{-3}\\right) = ...\\)\n\n\\(X_2 = -3.0000 + j1.7321\\):\n\n\\(|X_2| = \\sqrt{(-3)^2 + 1.7321^2} = ...\\)\n\\(\\angle{X_2} = \\arctan\\left(\\frac{1.7321}{-3}\\right) = ...\\)\n\n\\(X_3 = -3\\) is a real number, negative\n\n\\(|X_3| = 3\\)\n\\(\\angle{X_3} = \\pi\\)\n\n\\(X_4 = X_{4-N} = X_{-2} = X_{2}^*\\)\n\n\\(|X_4| = |X_2| = ...\\)\n\\(\\angle{X_4} = -\\angle{X_2} = ...\\)\n\n\\(X_5 = X_{5-N} = X_{-1} = X_{1}^*\\)\n\n\\(|X_5| = |X_1| = ...\\)\n\\(\\angle{X_5} = -\\angle{X_1} = ...\\)\n\n\nCompute these values and replace in the formula.\n\n\n\n\n\n\nComplex conjugates\n\n\n\nIt is not a coincidence that \\(X_4 = X_2^*\\) and \\(X_5 = X_1^*\\). If the signal \\(x[n]\\) has real values, then there is always this complex conjugate property, because the DFT coefficients are even: \\[X_k = X_{k-N} = X_{N-k}^*\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-5",
    "href": "Ex04_Fourier.html#exercise-5",
    "title": "6  The Fourier Transforms",
    "section": "6.5 Exercise 5",
    "text": "6.5 Exercise 5\nConsider a periodic signal \\(x[n]\\) with period \\(N=5\\) and the DFT coefficients:\n\\(X_k\\) = [15.0000 + 0.0000i , -2.5000 + 3.4410i , -2.5000 + 0.8123i , -2.5000 - 0.8123i , -2.5000 - 3.4410i]\nWrite \\(x[n]\\) as a sum of sinusoids.\n\nSolution\nThis is similar to the previous exercise, but we have \\(N=5\\), so we use the formula for odd \\(N\\).\nWe have:\n\n\\(X_0 = 15\\): is a real number, positive\n\n\\(|X_0| = 15\\)\n\\(\\angle{X_0} = 0\\)\n\n\\(X_1 = -2.5000 + j3.4410\\):\n\n\\(|X_1| = \\sqrt{(-2.5)^2 + 3.4410^2} = ...\\)\n\\(\\angle{X_1} = \\arctan\\left(\\frac{3.4410}{-2.5}\\right) = ...\\)\n\n\\(X_2 = -2.5000 + j0.8123\\):\n\n\\(|X_2| = \\sqrt{(-2.5)^2 + 0.8123^2} = ...\\)\n\\(\\angle{X_2} = \\arctan\\left(\\frac{0.8123}{-2.5}\\right) = ...\\)\n\n\nCompute these values and replace in the formula.",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-6",
    "href": "Ex04_Fourier.html#exercise-6",
    "title": "6  The Fourier Transforms",
    "section": "6.6 Exercise 6",
    "text": "6.6 Exercise 6\nFind the DFT coefficients of the periodic signal with period \\(\\{1, 1, 0, 0\\}\\), and write the signal as a sum of sinusoidal components.\n\nSolution\nFirst, we use the definition of the DFT to compute the DFT coefficients.: \\[X_k = \\sum_{n=0}^{N-1} x[n] e^{-j 2\\pi \\frac{k}{N} n}, k=0,1,...,N-1\\]\nHere \\(N=4\\), so we have to compute 4 sums \\(X_k\\), each with 4 terms.\n\\[\\begin{aligned}\nX_0 &= \\sum_{n=0}^{3} x[n] e^{-j 2\\pi \\frac{0}{4} n}\\\\\n&= x[0] + x[1] + x[2] + x[3] \\\\\n&= 2\n\\end{aligned}\\]\n\\[\\begin{aligned}\nX_1 &= \\sum_{n=0}^{3} x[n] e^{-j 2\\pi \\frac{1}{4} n}\\\\\n&= x[0] + x[1] e^{-j \\pi/2} + x[2] e^{-j \\pi} + x[3] e^{-j 3\\pi/2} \\\\\n&= 1 + 1 \\cdot (-j) + 0 \\cdot (-1) + 0 \\cdot j \\\\\n&= 1 - j\n\\end{aligned}\\]\n\\[\\begin{aligned}\nX_2 &= \\sum_{n=0}^{3} x[n] e^{-j 2\\pi \\frac{2}{4} n}\\\\\n&= x[0] + x[1] e^{-j \\pi} + x[2] e^{-j 2\\pi} + x[3] e^{-j 3\\pi} \\\\\n&= 1 + 1 \\cdot (-1) + 0 \\cdot 1 + 0 \\cdot (-1) \\\\\n&= 0\n\\end{aligned}\\]\n\\[\\begin{aligned}\nX_3 &= \\sum_{n=0}^{3} x[n] e^{-j 2\\pi \\frac{3}{4} n}\\\\\n&= x[0] + x[1] e^{-j 3\\pi/2} + x[2] e^{-j 3\\pi} + x[3] e^{-j 9\\pi/2}\\\\\n&= 1 + 1 \\cdot j + 0 \\cdot (-1) + 0 \\cdot (-j) \\\\\n&= 1 + j\n\\end{aligned}\\]\nNote that \\(X_3 = X_{3-N} = X_{-1} = X_1^*\\), as expected.\nSecond, we write the signal as a sum of sinusoidal components, using the formula for \\(N\\) even from the previous exercises: \\[x[n] = \\frac{1}{N} X_0 + \\frac{1}{N} \\sum_{k=1}^{N/2-1} 2 |X_k| \\cos\\left( 2\\pi \\frac{k}{N} n + \\angle{X_k}\\right) + \\frac{1}{N} |X_{N/2}| \\cos\\left( \\pi n + \\angle{X_{N/2}}\\right)\\]\nThe modulus and phase of the DFT coefficients are:\n\n\\(X_0 = 2\\): is a real number, positive\n\n\\(|X_0| = 2\\)\n\\(\\angle{X_0} = 0\\)\n\n\\(X_1 = 1 - j\\):\n\n\\(|X_1| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}\\)\n\\(\\angle{X_1} = \\arctan\\left(\\frac{-1}{1}\\right) = -\\frac{\\pi}{4}\\)\n\n\\(X_2 = 0\\):\n\n\\(|X_2| = 0\\)\n\\(\\angle{X_2}\\) undefined (doesn’t matter, because the coefficient is 0)\n\n\\(X_3 = 1 + j\\):\n\n\\(|X_3| = \\sqrt{1^2 + 1^2} = \\sqrt{2}\\)\n\\(\\angle{X_3} = \\arctan\\left(\\frac{1}{1}\\right) = \\frac{\\pi}{4}\\)\n\n\nTherefore we can write the signal as: \\[\\begin{aligned}\nx[n] &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 \\sqrt{2} \\cos\\left( 2\\pi \\frac{1}{4} n - \\frac{\\pi}{4}\\right)\\\\\n&= \\frac{1}{2} + \\frac{\\sqrt{2}}{2} \\cos\\left( \\frac{\\pi}{2} n - \\frac{\\pi}{4}\\right)\n\\end{aligned}\\]\nThe constant DC component is \\(\\frac{1}{2}\\), and there is one sinusoidal component with frequency \\(f=\\frac{1}{4}\\), amplitude \\(\\frac{\\sqrt{2}}{2}\\) and initial phase \\(-\\frac{\\pi}{4}\\).",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-7",
    "href": "Ex04_Fourier.html#exercise-7",
    "title": "6  The Fourier Transforms",
    "section": "6.7 Exercise 7",
    "text": "6.7 Exercise 7\nWrite the DFT calculation in Ex.5 as a matrix multiplication.\n\nSolution\nThis is just a straightforward illustration of the theory.\nNote that we can write the calculations for \\(X_k\\) as the following matrix-vector multiplication: \\[\\begin{bmatrix}\nX_0 \\\\\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & e^{-j \\pi/2} & e^{-j 2\\pi/2} & e^{-j 3\\pi/2} \\\\\n1 & e^{-j \\pi} & e^{-j 2\\pi} & e^{-j 3\\pi} \\\\\n1 & e^{-j 3\\pi/2} & e^{-j 6\\pi/2} & e^{-j 9\\pi/2}\n\\end{bmatrix} \\begin{bmatrix}\nx[0] \\\\\nx[1] \\\\\nx[2] \\\\\nx[3]\n\\end{bmatrix}\\]\nor, with the values replaced: \\[\\begin{aligned}\n\\begin{bmatrix}\nX_0 \\\\\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{bmatrix} &= \\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -j & -1 & j \\\\\n1 & -1 & 1 & -1 \\\\\n1 & j & -1 & -j\n\\end{bmatrix} \\begin{bmatrix}\n1 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n2 \\\\\n1-j \\\\\n0 \\\\\n1+j\n\\end{bmatrix}\n\\end{aligned}\\]\nWelcome to the world of linear algebra and orthogonal transformations!",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-8",
    "href": "Ex04_Fourier.html#exercise-8",
    "title": "6  The Fourier Transforms",
    "section": "6.8 Exercise 8",
    "text": "6.8 Exercise 8\nCompute \\(x[n]\\) in Ex.4 and Ex.5, in two ways:\n\nusing the definition formula\nusing the matrix form\n\n\nSolution\nWe have to compute the signal \\(x[n]\\) from the DFT coefficients \\(X_k\\) that are provided in the exercises.\nWe can achieve this in many ways:\n\nSince we have \\(x[n]\\) written as a sum of sinusoids, we can just give values to \\(n=0, 1, 2, 3, 4, 5\\) and compute the values.\nWe can use the definition formula of the Inverse DFT, IDFT: \\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k e^{j 2\\pi \\frac{k}{N} n}\\]\nWe can use the matrix form, which uses the transposed and conjugated matrix of the one used in the DFT calculation:\n\n\n6.8.0.1 Variant 1: using the sum of sinusoids\nWe have already written \\(x[n]\\) as a sum of sinusoids in the previous exercises.\nWe have to evaluate those expressions for \\(n=0, 1, 2, 3, ... , N-1\\).\n\n\n6.8.0.2 Variant 2: using the IDFT formula\nThe Inverse DFT (IDFT) formula is: \\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k e^{j 2\\pi \\frac{k}{N} n}, n=0,1,...,N-1\\] We have to compute \\(N\\) sums, each with \\(N\\) terms.\nWe will do this only for exercise 4: \\[\\begin{aligned}\nx[0] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 0} + X_1 e^{j 2\\pi \\frac{1}{6} 0} + X_2 e^{j 2\\pi \\frac{2}{6} 0} + X_3 e^{j 2\\pi \\frac{3}{6} 0} + X_4 e^{j 2\\pi \\frac{4}{6} 0} + X_5 e^{j 2\\pi \\frac{5}{6} 0})\\\\\n&= \\frac{1}{6} \\sum(X_0 + X_1 + X_2 + X_3 + X_4 + X_5)\\\\\n&= \\frac{1}{6} (21 - 3 - 3 - 3 - 3 - 3)\\\\\n&= 1\n\\end{aligned}\\] \\[\\begin{aligned}\nx[1] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 1} + X_1 e^{j 2\\pi \\frac{1}{6} 1} + X_2 e^{j 2\\pi \\frac{2}{6} 1} + X_3 e^{j 2\\pi \\frac{3}{6} 1} + X_4 e^{j 2\\pi \\frac{4}{6} 1} + X_5 e^{j 2\\pi \\frac{5}{6} 1})\\\\\n&= \\frac{1}{6} \\sum(X_0 e^{j 0} + X_1 e^{j \\pi/3} + X_2 e^{j 2\\pi/3} + X_3 e^{j \\pi} + X_4 e^{j 4\\pi/3} + X_5 e^{j 5\\pi/3})\\\\\n&= ...\n\\end{aligned}\\] \\[\\begin{aligned}\nx[2] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 2} + X_1 e^{j 2\\pi \\frac{1}{6} 2} + X_2 e^{j 2\\pi \\frac{2}{6} 2} + X_3 e^{j 2\\pi \\frac{3}{6} 2} + X_4 e^{j 2\\pi \\frac{4}{6} 2} + X_5 e^{j 2\\pi \\frac{5}{6} 2})\\\\\n&= \\frac{1}{6} \\sum(X_0 e^{j 0} + X_1 e^{j 2\\pi/3} + X_2 e^{j 4\\pi/3} + X_3 e^{j 2\\pi} + X_4 e^{j 8\\pi/3} + X_5 e^{j 10\\pi/3})\\\\\n&= ...\n\\end{aligned}\\] \\[\\begin{aligned}\nx[3] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 3} + X_1 e^{j 2\\pi \\frac{1}{6} 3} + X_2 e^{j 2\\pi \\frac{2}{6} 3} + X_3 e^{j 2\\pi \\frac{3}{6} 3} + X_4 e^{j 2\\pi \\frac{4}{6} 3} + X_5 e^{j 2\\pi \\frac{5}{6} 3})\\\\\n&= \\frac{1}{6} \\sum(X_0 e^{j 0} + X_1 e^{j \\pi} + X_2 e^{j 2\\pi} + X_3 e^{j 3\\pi} + X_4 e^{j 4\\pi} + X_5 e^{j 5\\pi})\\\\\n&= \\frac{1}{6} (X_0 - X_1 + X_2 - X_3 + X_4 - X_5)\\\\\n&= 4\n\\end{aligned}\\] \\[\\begin{aligned}\nx[4] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 4} + X_1 e^{j 2\\pi \\frac{1}{6} 4} + X_2 e^{j 2\\pi \\frac{2}{6} 4} + X_3 e^{j 2\\pi \\frac{3}{6} 4} + X_4 e^{j 2\\pi \\frac{4}{6} 4} + X_5 e^{j 2\\pi \\frac{5}{6} 4})\\\\\n&= \\frac{1}{6} \\sum(X_0 e^{j 0} + X_1 e^{j 4\\pi/3} + X_2 e^{j 8\\pi/3} + X_3 e^{j 4\\pi} + X_4 e^{j 16\\pi/3} + X_5 e^{j 20\\pi/3})\\\\\n&= ...\n\\end{aligned}\\] \\[\\begin{aligned}\nx[5] &= \\frac{1}{6} \\sum(X_0 e^{j 2\\pi \\frac{0}{6} 5} + X_1 e^{j 2\\pi \\frac{1}{6} 5} + X_2 e^{j 2\\pi \\frac{2}{6} 5} + X_3 e^{j 2\\pi \\frac{3}{6} 5} + X_4 e^{j 2\\pi \\frac{4}{6} 5} + X_5 e^{j 2\\pi \\frac{5}{6} 5})\\\\\n&= \\frac{1}{6} \\sum(X_0 e^{j 0} + X_1 e^{j 5\\pi/3} + X_2 e^{j 10\\pi/3} + X_3 e^{j 5\\pi} + X_4 e^{j 20\\pi/3} + X_5 e^{j 25\\pi/3})\\\\\n&= ...\n\\end{aligned}\\]\n\n\n\n6.8.1 Variant 3: using the matrix form\nNote that the preceding calculations can be written as a matrix-vector multiplication:\n\nfor exercise 4: \\[\\begin{bmatrix}\nx[0] \\\\\nx[1] \\\\\nx[2] \\\\\nx[3] \\\\\nx[4] \\\\\nx[5]\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & e^{j \\pi/6} & e^{j 2\\pi/6} & e^{j 3\\pi/6} & e^{j 4\\pi/6} & e^{j 5\\pi/6} \\\\\n1 & e^{j 2\\pi/6} & e^{j 4\\pi/6} & e^{j 6\\pi/6} & e^{j 8\\pi/6} & e^{j 10\\pi/6} \\\\\n1 & e^{j 3\\pi/6} & e^{j 6\\pi/6} & e^{j 9\\pi/6} & e^{j 12\\pi/6} & e^{j 15\\pi/6} \\\\\n1 & e^{j 4\\pi/6} & e^{j 8\\pi/6} & e^{j 12\\pi/6} & e^{j 16\\pi/6} & e^{j 20\\pi/6} \\\\\n1 & e^{j 5\\pi/6} & e^{j 10\\pi/6} & e^{j 15\\pi/6} & e^{j 20\\pi/6} & e^{j 25\\pi/6}\n\\end{bmatrix} \\begin{bmatrix}\nX_0 \\\\\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\nX_4 \\\\\nX_5\n\\end{bmatrix}\\]\n\nThe matrix is fixed (depends only on \\(N=6\\), not on the signal), so the multiplication is straightforward.\nWe won’t actually compute this, by hand. We could use MATLAB or Python to do it. Note that there is an even faster way to compute this, using the Fast Fourier Transform (FFT) algorithm.\nFor exercise 5, the matrix is the same, but with \\(N=5\\), so we have a \\(5 \\times 5\\) matrix. \\[\\begin{bmatrix}\nx[0] \\\\\nx[1] \\\\\nx[2] \\\\\nx[3] \\\\\nx[4]\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & e^{j \\pi/5} & e^{j 2\\pi/5} & e^{j 3\\pi/5} & e^{j 4\\pi/5} \\\\\n1 & e^{j 2\\pi/5} & e^{j 4\\pi/5} & e^{j 6\\pi/5} & e^{j 8\\pi/5} \\\\\n1 & e^{j 3\\pi/5} & e^{j 6\\pi/5} & e^{j 9\\pi/5} & e^{j 12\\pi/5} \\\\\n1 & e^{j 4\\pi/5} & e^{j 8\\pi/5} & e^{j 12\\pi/5} & e^{j 16\\pi/5}\n\\end{bmatrix} \\begin{bmatrix}\nX_0 \\\\\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\nX_4\n\\end{bmatrix}\\]",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-9",
    "href": "Ex04_Fourier.html#exercise-9",
    "title": "6  The Fourier Transforms",
    "section": "6.9 Exercise 9",
    "text": "6.9 Exercise 9\nA signal \\(x[n]\\) has a Z transform with one pole \\(p_1 = -0.5\\) and one zero \\(z_1 = 0.9\\). It is known that at \\(\\omega = \\pi\\), the modulus of the Fourier transform is \\(|X(\\omega=\\pi)| = 1\\).\n\n\nFind the signals’s Z transform \\(X(z)\\)\n\n\nCompute the expression of \\(|X(\\omega)|\\) and \\(\\angle X(\\omega)\\)\n\n\nFind the values \\(|X(\\frac{\\pi}{2})|\\), \\(|X(\\frac{-\\pi}{2})|\\) and \\(|X(0)|\\)\n\n\nSketch \\(|X(\\omega)|\\)\n\n\n\nSolution\nTODO",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  },
  {
    "objectID": "Ex04_Fourier.html#exercise-10",
    "href": "Ex04_Fourier.html#exercise-10",
    "title": "6  The Fourier Transforms",
    "section": "6.10 Exercise 10",
    "text": "6.10 Exercise 10\nDesign the pole-zero plot of a signal with:\n\nlow frequency content\nfrequency content around the frequency \\(\\omega = \\frac{\\pi}{2}\\)\n\n\nSolution\nTODO",
    "crumbs": [
      "Solved Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fourier Transforms</span>"
    ]
  }
]